{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with GPT architecture Using Pre-setting Vocab and Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working path to the current file\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from datasets.data_loader import *\n",
    "from models.GPT import *\n",
    "from models.loss import InfoNCELoss\n",
    "from models.trfm import *\n",
    "from utils.utils import *\n",
    "from utils.plot_figures import *\n",
    "from utils.metrics import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "train_ce_loss_history = []\n",
    "test_ce_loss_history = []\n",
    "train_info_loss_history = []\n",
    "test_info_loss_history = []\n",
    "\n",
    "log_dir = './logs/'\n",
    "save_best_weight_path = './checkpoints/'\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the file\n",
    "train_data = pd.read_csv('./data/train_contrastive_dataset.csv')\n",
    "test_data = pd.read_csv('./data/test_contrastive_dataset.csv')\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "epoch = 10\n",
    "InfoNCEloss_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and dataloader\n",
    "# train_dataset = Contrastive_Seq2seqDataset(train_data, vocab, MAX_LEN)\n",
    "# test_dataset = Contrastive_Seq2seqDataset(test_data, vocab, MAX_LEN)\n",
    "train_dataset = Contrastive_Seq2seqDataset_random(train_data, vocab, MAX_LEN)\n",
    "test_dataset = Contrastive_Seq2seqDataset_random(test_data, vocab, MAX_LEN)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# create the model\n",
    "config = GPTConfig(vocab_size=charlen, block_size=220, num_props=24)\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "# load model\n",
    "trfm = TrfmSeq2seq(charlen, 256, charlen, 4).to(device)\n",
    "trfm.load_state_dict(torch.load('./model_hub/trfm_new_2_10000.pkl'))\n",
    "trfm.eval()\n",
    "\n",
    "# set trfm gradient to false which won't be updated\n",
    "for param in trfm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# loss\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, trfm, train_dataloader, loss_func, optim, device, weight=0.1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_crossentropy = 0\n",
    "    total_loss_infonce = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    # tau = 1.0\n",
    "    # min_tau = 0.1\n",
    "    # decay_rate = 0.999\n",
    "    for i, (zeo, syn, tgt, positive_smiles, negative_smiles) in enumerate(tqdm(train_dataloader)):\n",
    "        zeo = zeo.to(device)\n",
    "        syn = syn.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        positive_smiles = positive_smiles.to(device)\n",
    "        negative_smiles = negative_smiles.to(device)\n",
    "        \n",
    "        # concat zeo and syn as the input (prop)\n",
    "        synthesis_condition = torch.cat([zeo, syn], dim=-1)\n",
    "        tgt_input = tgt[:, :-1].contiguous()\n",
    "        tgt_label = tgt[:, 1:].contiguous()\n",
    "        \n",
    "        # forward\n",
    "        optim.zero_grad()\n",
    "        logits = model(idx=tgt_input, prop=synthesis_condition)\n",
    "        \n",
    "        # Compute CrossEntropy Loss\n",
    "        logits_reshaped = logits.view(-1, logits.size(-1))\n",
    "        tgt_label_reshaped = tgt_label.view(-1)\n",
    "        loss_ce = loss_func(logits_reshaped, tgt_label_reshaped)\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "        num_words = (tgt_label != PAD).sum().item()\n",
    "        \n",
    "        # Gumbel-Softmax Sampling\n",
    "        # logits: [batch_size, seq_len, vocab_size]\n",
    "        # tau = max(min_tau, tau * decay_rate)\n",
    "        samples = F.gumbel_softmax(logits, tau=1.0, hard=True)\n",
    "        # samples = F.gumbel_softmax(logits, tau, hard=True)\n",
    "        samples_indices = samples.argmax(dim=-1) # [batch_size, seq_len]\n",
    "        # add the start token to the samples\n",
    "        stared_token = torch.ones(samples_indices.size(0), 1, dtype=torch.long).fill_(SOS).to(device)\n",
    "        samples_indices = torch.cat([stared_token, samples_indices], dim=-1)\n",
    "        \n",
    "        # Compute InfoNCE Loss\n",
    "        loss_infonce = InfoNCELoss(samples_indices, positive_smiles, negative_smiles, trfm, temperature=0.07)\n",
    "        \n",
    "        # combine the two loss\n",
    "        loss = loss_ce + weight * loss_infonce\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_loss_crossentropy += loss_ce.item()\n",
    "        total_loss_infonce += loss_infonce.item()\n",
    "        total_acc += num_correct.sum().item()\n",
    "        total_num += num_words\n",
    "    return total_loss / len(train_dataloader), total_acc / total_num, total_loss_crossentropy / len(train_dataloader), total_loss_infonce / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, trfm, test_dataloader, loss_func, device, weight=0.1):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_crossentropy = 0\n",
    "    total_loss_infonce = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (zeo, syn, tgt, positive_smiles, negative_smiles) in enumerate(tqdm(test_dataloader)):\n",
    "            zeo = zeo.to(device)\n",
    "            syn = syn.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            positive_smiles = positive_smiles.to(device)\n",
    "            negative_smiles = negative_smiles.to(device)\n",
    "            \n",
    "            # concat zeo and syn as the input (prop)\n",
    "            synthesis_condition = torch.cat([zeo, syn], dim=-1)\n",
    "            tgt_input = tgt[:, :-1].contiguous()\n",
    "            tgt_label = tgt[:, 1:].contiguous()\n",
    "            \n",
    "            # forward\n",
    "            logits = model(idx=tgt_input, prop=synthesis_condition)\n",
    "            \n",
    "            # Compute CrossEntropy Loss\n",
    "            logits_reshaped = logits.view(-1, logits.size(-1))\n",
    "            tgt_label_reshaped = tgt_label.view(-1)\n",
    "            loss_ce = loss_func(logits_reshaped, tgt_label_reshaped)\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "            num_words = (tgt_label != PAD).sum().item()\n",
    "            \n",
    "            # Gumbel-Softmax Sampling\n",
    "            # logits: [batch_size, seq_len, vocab_size]\n",
    "            samples = F.gumbel_softmax(logits, tau=1.0, hard=True)\n",
    "            samples_indices = samples.argmax(dim=-1) # [batch_size, seq_len]\n",
    "            # add the start token to the samples\n",
    "            stared_token = torch.ones(samples_indices.size(0), 1, dtype=torch.long).fill_(SOS).to(device)\n",
    "            samples_indices = torch.cat([stared_token, samples_indices], dim=-1)\n",
    "            \n",
    "            # Compute InfoNCE Loss\n",
    "            loss_infonce = InfoNCELoss(samples_indices, positive_smiles, negative_smiles, trfm, temperature=0.2)\n",
    "            \n",
    "            # combine the two loss\n",
    "            loss = loss_ce + weight * loss_infonce\n",
    "            \n",
    "            # statistics\n",
    "            total_loss += loss.item()\n",
    "            total_loss_crossentropy += loss_ce.item()\n",
    "            total_loss_infonce += loss_infonce.item()\n",
    "            total_acc += num_correct.sum().item()\n",
    "            total_num += num_words\n",
    "    return total_loss / len(test_dataloader), total_acc / total_num, total_loss_crossentropy / len(test_dataloader), total_loss_infonce / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# for i in range(epoch):\n",
    "#     train_loss, train_acc, train_ce, train_info = train(model, trfm, train_dataloader, loss_func, optim, device)\n",
    "#     # train_loss, train_acc, train_ce, train_info = train(model, trfm, test_dataloader, loss_func, optim, device)\n",
    "#     train_loss_history.append(train_loss)\n",
    "#     train_acc_history.append(train_acc)\n",
    "#     train_ce_loss_history.append(train_ce)\n",
    "#     train_info_loss_history.append(train_info)\n",
    "#     print('epoch: %d, train loss: %.4f, train acc: %.4f, train crossentropy loss: %.4f, train infonce loss: %.4f' % (i, train_loss, train_acc, train_ce, train_info))\n",
    "#     test_loss, test_acc, test_ce, test_info = evaluate(model, trfm, test_dataloader, loss_func, device)\n",
    "#     test_loss_history.append(test_loss)\n",
    "#     test_acc_history.append(test_acc)\n",
    "#     test_ce_loss_history.append(test_ce)\n",
    "#     test_info_loss_history.append(test_info)\n",
    "#     print('epoch: %d test loss: %.4f, test acc: %.4f, test crossentropy loss: %.4f, test infonce loss: %.4f' % (i, test_loss, test_acc, test_ce, test_info))\n",
    "#     if i == 0:\n",
    "#         best_acc = test_acc\n",
    "#     if test_acc > best_acc:\n",
    "#         best_acc = test_acc\n",
    "#         torch.save(model.state_dict(), save_best_weight_path + 'best_GPT_contrastive_model.pth')\n",
    "#     torch.save(model.state_dict(), save_best_weight_path + 'last_GPT_contrastive_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = GPT(config).to(device)\n",
    "model.load_state_dict(torch.load('./checkpoints/best_GPT_contrastive_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt(model, start_sequence, condition_props, max_length, vocab, device, temperature=0.5, top_k=0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation process for a GPT model.\n",
    "\n",
    "    Args:\n",
    "        model (GPT): The pre-trained GPT model for token generation.\n",
    "        start_sequence (torch.Tensor): The initial sequence to start generation (batch_size, seq_length).\n",
    "        condition_props (torch.Tensor): The conditional property vector (batch_size, num_props).\n",
    "        max_length (int): The maximum length of the generated sequence.\n",
    "        vocab: The vocabulary object for encoding and decoding SMILES strings.\n",
    "        device (torch.device): The device on which to run the generation.\n",
    "        temperature (float): Temperature parameter for sampling; higher values increase randomness.\n",
    "        top_k (int): Limits sampling to top-k logits; if 0, no top-k sampling is applied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated SMILES strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = start_sequence.size(0)\n",
    "    generated_sequences = start_sequence.clone().to(device)  # Clone and move to device\n",
    "\n",
    "    for _ in range(max_length - start_sequence.size(1)):\n",
    "        # Get the current sequence length\n",
    "        current_length = generated_sequences.size(1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(generated_sequences, condition_props)  # (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Extract the logits for the last time step\n",
    "        next_token_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "            mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "            mask.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)\n",
    "            next_token_logits = mask\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Sample from the probability distribution\n",
    "        next_token = torch.multinomial(next_token_probs, num_samples=1)  # (batch_size, 1)\n",
    "        \n",
    "        # Get the most likely next token\n",
    "        # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the generated token to the sequence\n",
    "        generated_sequences = torch.cat([generated_sequences, next_token], dim=1)\n",
    "\n",
    "        # Check if all sequences have reached the end token\n",
    "        if all(next_token[i].item() == EOS for i in range(batch_size)):\n",
    "            break\n",
    "\n",
    "    # Decode the generated sequences into SMILES strings\n",
    "    generated_smiles = []\n",
    "    for seq in generated_sequences:\n",
    "        # Convert indices to characters, ignoring padding and start tokens\n",
    "        # check if the generated sequence contains the end token, if meet, stop decoding\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS and idx.item() != UNK:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the smiles for the test dataset\n",
    "generated_smile = []\n",
    "target_smile = []\n",
    "for i, (zeo, syn, tgt, positive_smiles, negative_smiles) in enumerate(tqdm(test_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_gpt(model, tgt[:, :2], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile.extend(tgt_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics\n",
    "print('Validity rate:', validity_rate(generated_smiles))\n",
    "print('Uniqueness rate:', uniqueness_rate(generated_smiles))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('Reconstructability rate:', reconstructability_rate(generated_smiles, target_smile))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('IntDiv:', IntDiv(generated_smiles))\n",
    "# print('KL-divergence:', KL_divergence(target_smile), generated_smile))\n",
    "print('FCD score:', FCD_score(target_smile, generated_smile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and acc\n",
    "plot_loss(train_loss_history, test_loss_history, 'GPT_Contrastive_Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the history to the csv file in log folder\n",
    "history = pd.DataFrame({'train_loss': train_loss_history, 'train_acc': train_acc_history, 'test_loss': test_loss_history, 'test_acc': test_acc_history, 'train_ce_loss': train_ce_loss_history, 'test_ce_loss': test_ce_loss_history, 'train_info_loss': train_info_loss_history, 'test_info_loss': test_info_loss_history})\n",
    "history.to_csv(log_dir + 'GPT_Contrastive_History.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
