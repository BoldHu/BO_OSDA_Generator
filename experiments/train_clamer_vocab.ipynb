{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Clamer Model with Pre-setting Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the directory of the script\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from models.clamer import *\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from utils.plot_figures import *\n",
    "from utils.metrics import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "log_dir = './logs/'\n",
    "save_best_weight_path = './checkpoints/'\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and convert to the format we need\n",
    "train_smiles = read_strings('./data/train_smiles.csv', idx=False)\n",
    "train_zeo = read_vec('./data/train_zeo.csv', idx=False)\n",
    "train_syn = read_vec('./data/train_syn.csv', idx=False)\n",
    "train_codes = read_strings('./data/train_codes.csv', idx=False)\n",
    "test_smiles = read_strings('./data/test_smiles.csv', idx=False)\n",
    "test_zeo = read_vec('./data/test_zeo.csv', idx=False)\n",
    "test_syn = read_vec('./data/test_syn.csv', idx=False)\n",
    "test_codes = read_strings('./data/test_codes.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "d_model = 128\n",
    "head = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and dataloader\n",
    "train_dataset = Seq2seqDataset(train_zeo, train_syn, train_smiles, vocab)\n",
    "test_dataset = Seq2seqDataset(test_zeo, test_syn, test_smiles, vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# load model\n",
    "model = GptCovd(d_model=d_model, charlen=charlen, device=device, head=head).to(device, non_blocking=True)\n",
    "# loss\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "# sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))  #  打印参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, train_dataloader, loss_func, optim, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for i, (zeo, syn, tgt) in enumerate(tqdm(train_dataloader)):\n",
    "        zeo = zeo.to(device)\n",
    "        syn = syn.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1].contiguous()\n",
    "        tgt_label = tgt[:, 1:].contiguous()\n",
    "        \n",
    "        # forward\n",
    "        optim.zero_grad()\n",
    "        # ignore the first two tokens becauese thay are condition tokens\n",
    "        output = model(zeo, syn, tgt_input)\n",
    "        output = output[:, 2:, :].to(device, non_blocking=True).contiguous()\n",
    "        loss = loss_func(output.view(-1, output.size(-1)), tgt_label.view(-1))\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        pred = torch.argmax(output, dim=-1)\n",
    "        num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "        num_words = (tgt_label != PAD).sum().item()\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += num_correct.sum().item()\n",
    "        total_num += num_words\n",
    "    return total_loss / len(train_dataloader), total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (zeo, syn, tgt) in enumerate(tqdm(test_dataloader)):\n",
    "            zeo = zeo.to(device)\n",
    "            syn = syn.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1].contiguous()\n",
    "            tgt_label = tgt[:, 1:].contiguous()\n",
    "            # forward\n",
    "            output = model(zeo, syn, tgt_input)\n",
    "            output = output[:, 2:, :].to(device, non_blocking=True).contiguous()\n",
    "            loss = loss_func(output.view(-1, output.size(-1)), tgt_label.view(-1))\n",
    "            # calculate the accuracy\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "            num_words = (tgt_label != PAD).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_acc += num_correct.sum().item()\n",
    "            total_num += num_words\n",
    "    return total_loss / len(test_dataloader), total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "for i in range(epoch):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_func, optim, device)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.4f' % (i, train_loss, train_acc))\n",
    "    test_loss, test_acc = evaluate(model, test_dataloader, loss_func, device)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_acc_history.append(test_acc)\n",
    "    print('epoch: %d test loss: %.4f, test acc: %.4f' % (i, test_loss, test_acc))\n",
    "    if i == 0:\n",
    "        best_acc = test_acc\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), save_best_weight_path + 'best_Clamer_model.pth')\n",
    "    torch.save(model.state_dict(), save_best_weight_path + 'last_Clamer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clamer(model, start_sequence, zeo, syn, max_length, vocab, device, temperature=1.0, top_k=0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation process for a GPT model.\n",
    "\n",
    "    Args:\n",
    "        model (CLAMER): The pre-trained GPT model for token generation.\n",
    "        start_sequence (torch.Tensor): The initial sequence to start generation (batch_size, seq_length).\n",
    "        zeo (torch.Tensor): The zeolite condition tensor.\n",
    "        syn (torch.Tensor): The synthesis condition tensor.\n",
    "        max_length (int): The maximum length of the generated sequence.\n",
    "        vocab: The vocabulary object for encoding and decoding SMILES strings.\n",
    "        device (torch.device): The device on which to run the generation.\n",
    "        temperature (float): Temperature parameter for sampling; higher values increase randomness.\n",
    "        top_k (int): Limits sampling to top-k logits; if 0, no top-k sampling is applied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated SMILES strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = start_sequence.size(0)\n",
    "    generated_sequences = start_sequence.clone().to(device)  # Clone and move to device\n",
    "    # expand generated_sequences to the max_length with padding tokens\n",
    "    padding = torch.full((batch_size, max_length - generated_sequences.size(1)), PAD, dtype=torch.long).to(device)\n",
    "    generated_sequences_input = torch.cat([generated_sequences, padding], dim=1)[:,:-1].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the\n",
    "        for current_len in range(max_length - start_sequence.size(1)):\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            logits = model(zeo, syn, generated_sequences_input)\n",
    "            # Extract the logits for the last time step\n",
    "            next_token_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "                mask.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)\n",
    "                next_token_logits = mask\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)  # (batch_size, 1)\n",
    "            \n",
    "            # Get the most likely next token\n",
    "            # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # Append the generated token to the sequence\n",
    "            generated_sequences = torch.cat([generated_sequences, next_token], dim=1)\n",
    "            # update the input sequence for the next iteration\n",
    "            generated_sequences_input[:, current_len] = next_token.squeeze(1)\n",
    "\n",
    "            # Check if all sequences have reached the end token\n",
    "            if all(next_token[i].item() == EOS for i in range(batch_size)):\n",
    "                break\n",
    "\n",
    "    # Decode the generated sequences into SMILES strings\n",
    "    generated_smiles = []\n",
    "    for seq in generated_sequences:\n",
    "        # Convert indices to characters, ignoring padding and start tokens\n",
    "        # check if the generated sequence contains the end token, if meet, stop decoding\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the smiles for the test dataset\n",
    "generated_smile = []\n",
    "target_smile = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(test_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    generated_smiles = generate_clamer(model, tgt[:, :5], zeo, syn, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile.extend(tgt_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the generated smiles are :', generated_smile[1:3])\n",
    "print('the target smiles are :', target_smile[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics\n",
    "print('Validity rate:', validity_rate(generated_smiles))\n",
    "print('Uniqueness rate:', uniqueness_rate(generated_smiles))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('Reconstructability rate:', reconstructability_rate(generated_smiles, target_smile))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('IntDiv:', IntDiv(generated_smiles))\n",
    "# print('KL-divergence:', KL_divergence(target_smile), generated_smile))\n",
    "print('FCD score:', FCD_score(target_smile, generated_smile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and acc\n",
    "plot_loss(train_loss_history, test_loss_history, 'Clamer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
