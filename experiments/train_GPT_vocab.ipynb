{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with GPT architecture Using Pre-setting Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working path to the current file\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from models.GPT import *\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from utils.plot_figures import *\n",
    "from utils.metrics import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "log_dir = './logs/'\n",
    "save_best_weight_path = './checkpoints/'\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and convert to the format we need\n",
    "train_smiles = read_strings('./data/train_smiles.csv', idx=False)\n",
    "train_zeo = read_vec('./data/train_zeo.csv', idx=False)\n",
    "train_syn = read_vec('./data/train_syn.csv', idx=False)\n",
    "train_codes = read_strings('./data/train_codes.csv', idx=False)\n",
    "test_smiles = read_strings('./data/test_smiles.csv', idx=False)\n",
    "test_zeo = read_vec('./data/test_zeo.csv', idx=False)\n",
    "test_syn = read_vec('./data/test_syn.csv', idx=False)\n",
    "test_codes = read_strings('./data/test_codes.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epoch = 20\n",
    "seqlen = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and dataloader\n",
    "train_dataset = Seq2seqDataset(train_zeo, train_syn, train_smiles, vocab)\n",
    "test_dataset = Seq2seqDataset(test_zeo, test_syn, test_smiles, vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# create the model\n",
    "config = GPTConfig(vocab_size=charlen, block_size=220, num_props=24)\n",
    "model = GPT(config).to(device)\n",
    "# loss\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, train_dataloader, loss_func, optim, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for i, (zeo, syn, tgt) in enumerate(tqdm(train_dataloader)):\n",
    "        zeo = zeo.to(device)\n",
    "        syn = syn.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        # concat zeo and syn as the input (prop)\n",
    "        synthesis_condition = torch.cat([zeo, syn], dim=-1)\n",
    "        tgt_input = tgt[:, :-1].contiguous()\n",
    "        tgt_label = tgt[:, 1:].contiguous()\n",
    "        \n",
    "        # forward\n",
    "        optim.zero_grad()\n",
    "        output = model(idx=tgt_input, prop=synthesis_condition)\n",
    "        loss = loss_func(output.view(-1, output.size(-1)), tgt_label.view(-1))\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        pred = torch.argmax(output, dim=-1)\n",
    "        num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "        num_words = (tgt_label != PAD).sum().item()\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += num_correct.sum().item()\n",
    "        total_num += num_words\n",
    "    return total_loss / len(train_dataloader), total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (zeo, syn, tgt) in enumerate(tqdm(test_dataloader)):\n",
    "            zeo = zeo.to(device)\n",
    "            syn = syn.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            # concat zeo and syn as the input (prop)\n",
    "            synthesis_condition = torch.cat([zeo, syn], dim=-1)\n",
    "            tgt_input = tgt[:, :-1].contiguous()\n",
    "            tgt_label = tgt[:, 1:].contiguous()\n",
    "            # forward\n",
    "            output = model(idx=tgt_input, prop=synthesis_condition)\n",
    "            loss = loss_func(output.view(-1, output.size(-1)), tgt_label.view(-1))\n",
    "            # calculate the accuracy\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            num_correct = (pred == tgt_label) & (tgt_label != PAD)\n",
    "            num_words = (tgt_label != PAD).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_acc += num_correct.sum().item()\n",
    "            total_num += num_words\n",
    "    return total_loss / len(test_dataloader), total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "for i in range(epoch):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_func, optim, device)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.4f' % (i, train_loss, train_acc))\n",
    "    test_loss, test_acc = evaluate(model, test_dataloader, loss_func, device)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_acc_history.append(test_acc)\n",
    "    print('epoch: %d test loss: %.4f, test acc: %.4f' % (i, test_loss, test_acc))\n",
    "    if i == 0:\n",
    "        best_acc = test_acc\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), save_best_weight_path + 'best_GPT_model.pth')\n",
    "    torch.save(model.state_dict(), save_best_weight_path + 'last_GPT_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt(model, start_sequence, condition_props, max_length, vocab, device, temperature=1.0, top_k=0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation process for a GPT model.\n",
    "\n",
    "    Args:\n",
    "        model (GPT): The pre-trained GPT model for token generation.\n",
    "        start_sequence (torch.Tensor): The initial sequence to start generation (batch_size, seq_length).\n",
    "        condition_props (torch.Tensor): The conditional property vector (batch_size, num_props).\n",
    "        max_length (int): The maximum length of the generated sequence.\n",
    "        vocab: The vocabulary object for encoding and decoding SMILES strings.\n",
    "        device (torch.device): The device on which to run the generation.\n",
    "        temperature (float): Temperature parameter for sampling; higher values increase randomness.\n",
    "        top_k (int): Limits sampling to top-k logits; if 0, no top-k sampling is applied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated SMILES strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = start_sequence.size(0)\n",
    "    generated_sequences = start_sequence.clone().to(device)  # Clone and move to device\n",
    "\n",
    "    for _ in range(max_length - start_sequence.size(1)):\n",
    "        # Get the current sequence length\n",
    "        current_length = generated_sequences.size(1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(generated_sequences, condition_props)  # (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Extract the logits for the last time step\n",
    "        next_token_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "            mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "            mask.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)\n",
    "            next_token_logits = mask\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Sample from the probability distribution\n",
    "        next_token = torch.multinomial(next_token_probs, num_samples=1)  # (batch_size, 1)\n",
    "        \n",
    "        # Get the most likely next token\n",
    "        # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the generated token to the sequence\n",
    "        generated_sequences = torch.cat([generated_sequences, next_token], dim=1)\n",
    "\n",
    "        # Check if all sequences have reached the end token\n",
    "        if all(next_token[i].item() == EOS for i in range(batch_size)):\n",
    "            break\n",
    "\n",
    "    # Decode the generated sequences into SMILES strings\n",
    "    generated_smiles = []\n",
    "    for seq in generated_sequences:\n",
    "        # Convert indices to characters, ignoring padding and start tokens\n",
    "        # check if the generated sequence contains the end token, if meet, stop decoding\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the smiles for the test dataset\n",
    "generated_smile = []\n",
    "target_smile = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(test_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_gpt(model, tgt[:, :2], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile.extend(tgt_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics\n",
    "print('Validity rate:', validity_rate(generated_smiles))\n",
    "print('Uniqueness rate:', uniqueness_rate(generated_smiles))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('Reconstructability rate:', reconstructability_rate(generated_smiles, target_smile))\n",
    "print('Novelty rate:', novelty_rate(generated_smiles, target_smile))\n",
    "print('IntDiv:', IntDiv(generated_smiles))\n",
    "# print('KL-divergence:', KL_divergence(target_smile), generated_smile))\n",
    "print('FCD score:', FCD_score(target_smile, generated_smile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and acc\n",
    "plot_loss(train_loss_history, test_loss_history, 'GPT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
