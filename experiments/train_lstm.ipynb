{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hudongcheng/Desktop/bo_osda_generator\n"
     ]
    }
   ],
   "source": [
    "# change to the directory of the working file\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the necessary modules\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from models.lstm import LSTM_Variant\n",
    "from utils.plot_figures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the charset(inculde begin end and pad char) achieved from dataset : ?P25$]FO-S.Hc=71(ln63NC4[+)^@\n",
      "the total num of charset is : 29\n",
      "cuda\n",
      "total parameters: 0.96M\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "train_smiles = read_strings('./data/train_smiles.csv', idx=False)\n",
    "train_zeo = read_vec('./data/train_zeo.csv', idx=False)\n",
    "train_syn = read_vec('./data/train_syn.csv', idx=False)\n",
    "train_codes = read_strings('./data/train_codes.csv', idx=False)\n",
    "test_smiles = read_strings('./data/test_smiles.csv', idx=False)\n",
    "test_zeo = read_vec('./data/test_zeo.csv', idx=False)\n",
    "test_syn = read_vec('./data/test_syn.csv', idx=False)\n",
    "test_codes = read_strings('./data/test_codes.csv', idx=False)\n",
    "\n",
    "charset = '?P25$]FO-S.Hc=71(ln63NC4[+)^@'\n",
    "charlen = len(charset)\n",
    "print('the charset(inculde begin end and pad char) achieved from dataset :', charset)\n",
    "print('the total num of charset is :', charlen)\n",
    "# create the char to index and index to char dictionary\n",
    "char_to_index = dict((c, i) for i, c in enumerate(charset))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(charset))\n",
    "char_list = [k for k, v in char_to_index.items()]\n",
    "\n",
    "cudnn.benchmark = True\n",
    "lr = 2e-3\n",
    "batch_size = 512\n",
    "n_epoch = 2\n",
    "seqlen = 127\n",
    "\n",
    "manual_seed = 42\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "# load the training and testing data\n",
    "src_smiles, tgt_smiles = smiles_padding(train_smiles)\n",
    "tgt_seq = smiles_to_sequence(tgt_smiles, char_to_index)\n",
    "tgt_seq = torch.cat([torch.unsqueeze(seq, 0) for seq in tgt_seq]).long()\n",
    "src_smiles_test, tgt_smiles_test = smiles_padding(test_smiles)\n",
    "tgt_seq_test = smiles_to_sequence(tgt_smiles_test, char_to_index)\n",
    "tgt_seq_test = torch.cat([torch.unsqueeze(seq, 0) for seq in tgt_seq_test]).long()\n",
    "# create the dataset and dataloader\n",
    "train_dataset = SeqDataset(train_zeo, train_syn, tgt_seq)\n",
    "test_dataset = SeqDataset(test_zeo, test_syn, tgt_seq_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# load the model\n",
    "model = LSTM_Variant(charlen, embedding_dim=128, hidden_dim=256, conditional_synthesis_dim=24, num_layers=2, dropout=0.5)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))  # print the total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training and testing functions\n",
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    logits = logits / temperature  # (batch_size, vocab_size)\n",
    "    probs = F.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze(-1)  # (batch_size,)\n",
    "\n",
    "def train(epoches, model, iterator, optimizer, criterion, device):\n",
    "    # teaching forcing\n",
    "    epoch_loss = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for i, (zeo, syn, label) in enumerate(iterator):\n",
    "            # concat the zeo and syn\n",
    "            conditional_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "            conditional_synthesis, label = conditional_synthesis.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, seq_len = label.shape\n",
    "            \n",
    "            # 这里有问题，应该是输入去掉最后一个token，输出去掉第一个token\n",
    "            # forward pass\n",
    "            output, _ = model(conditional_synthesis, label[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            label = label[:, 1:].contiguous().view(-1)\n",
    "            # # forward pass\n",
    "            # output, _ = model(conditional_synthesis, label)\n",
    "            # output_dim = output.shape[-1] # vocab size\n",
    "            # # ignore the last token and reshape the output\n",
    "            # output = output[:, :-1, :].contiguous().view(-1, output_dim) # (batch_size * seq_len, vocab_size)\n",
    "            # # ignore the first token and reshape the label\n",
    "            # label = label[:, 1:].contiguous().view(-1) # (batch_size * seq_len)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            # print information\n",
    "            if i % 100 == 0:\n",
    "                print(f'Batch: {i}, Loss: {loss.item()}')\n",
    "        \n",
    "        # calculate the average loss\n",
    "        train_loss_list.append(epoch_loss / len(iterator))\n",
    "        test_loss = test(model, test_dataloader, criterion, device)\n",
    "        test_loss_list.append(test_loss)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Train Loss: {epoch_loss / len(iterator)}, Test Loss: {test_loss}')\n",
    "    return train_loss_list, test_loss_list\n",
    "\n",
    "def test(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    generated_sequences = []\n",
    "    with torch.no_grad():\n",
    "        for i, (zeo, syn, label) in enumerate(iterator):\n",
    "            # concat the zeo and syn\n",
    "            conditional_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "            conditional_synthesis, label = conditional_synthesis.to(device), label.to(device)\n",
    "            batch_size = conditional_synthesis.shape[0]\n",
    "            # set the initial hidden state\n",
    "            hidden = None\n",
    "            # set the initial input (batch_size, 1)\n",
    "            generated_sequence = label[:, :1] # (batch_size, 1)\n",
    "            batch_loss = 0\n",
    "            \n",
    "            # auto-regressive\n",
    "            for t in range(label.shape[1]):\n",
    "                # forward pass\n",
    "                current_token = generated_sequence[:, -1].unsqueeze(1) # (batch_size, 1)\n",
    "                output, hidden = model(conditional_synthesis, current_token, hidden) # (batch_size, 1, vocab_size)\n",
    "                \n",
    "                # sample for the next input\n",
    "                next_token = sample_from_logits(output.squeeze(1)) # (batch_size,)\n",
    "                generated_sequence = torch.cat([generated_sequence, next_token.unsqueeze(1)], dim=1) # (batch_size, t+2)\n",
    "                \n",
    "                # calculate the loss\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output.view(-1, output_dim) # (batch_size, vocab_size)\n",
    "                label_t = label[:, t]\n",
    "                loss = criterion(output, label_t)\n",
    "                batch_loss += loss.item()\n",
    "            \n",
    "            batch_loss /= label.shape[1]\n",
    "            total_loss += batch_loss\n",
    "            generated_sequences.append(generated_sequence)\n",
    "        \n",
    "        # calculate the average loss\n",
    "        total_loss /= len(iterator)\n",
    "        \n",
    "    # convert the generated sequences to smiles\n",
    "    generated_sequences = torch.cat(generated_sequences, dim=0).cpu().numpy()\n",
    "    generated_sequences = sequence_to_smiles(generated_sequences, index_to_char)\n",
    "    print(generated_sequences[:10])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 3.3549416065216064\n",
      "Batch: 100, Loss: 0.27437087893486023\n",
      "Batch: 200, Loss: 0.20944635570049286\n",
      "seq_list: [[27 22 18 ...  0  0  0]\n",
      " [27 22 15 ...  0  0  0]\n",
      " [27 22 24 ...  0  0  0]\n",
      " ...\n",
      " [27 22 15 ...  0  0  0]\n",
      " [27 22 22 ...  0  0  0]\n",
      " [27 22 16 ...  0  0  0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['^Cn1cc[n+](Cc2c(C)cc2)ccc1', '^C1C2C3C[N+](C)(CC4CC(C4)CCCC3)OC1', '^C[N+]1(C)CCCCC1', '^C[N+](C2)(CCN(CC)CCCCC2)C', '^c1ccccc1(C[N+](C)(C)C)CCC2', '^[n+]1(C)c(cccn1)c1ccccc1', '^C1[N+]2(C)CC(C)C=(CCC2C=C1)CC(C)CCC2', '^c1[n+](CC[n+]2ccn(c2C)C)cc(-2', '^C1CCC2CC[N+]21C1CC(C[N+]3(CCCC2)C)(C)CC1', '^C[n+]1(CCCCC)cccc1']\n",
      "Epoch: 0, Train Loss: 0.0, Test Loss: 1.9663331562097544\n",
      "Batch: 0, Loss: 0.1896256059408188\n",
      "Batch: 100, Loss: 0.16198010742664337\n",
      "Batch: 200, Loss: 0.15121105313301086\n",
      "seq_list: [[27 22 16 ...  0  0  0]\n",
      " [27 22 15 ...  0  0  0]\n",
      " [27 24 18 ...  0  0  0]\n",
      " ...\n",
      " [27 24 21 ...  0  0  0]\n",
      " [27 24 21 ...  0  0  0]\n",
      " [27 22 15 ...  0  0  0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['^C(C[N+]1(C2CC3CCCC(C32)C3)CCCC1)C', '^C1C[N+]2(CC(C1C1C[N+](CC)(CC3C2C3)CCN(C3C5C=CC(C4C2C1(C)C2)C)CC)CC)C[N+](CCC)(CC)CC', '^[n+]1(ccn(c1C)C)CCCC[n+]1c(C)n(C)c(C)c1C', '^c1(CCn2cc[n+](c2)C)C', '^C(C)[N+]1(CC)CC2[N+](CC3CC[N+](C)(CC)CC1C2)(CC)CC', '^C[N+]1(CC)C(C)CC(C)C2C3C(C[N+](CC)(C=C4C21)C)CC[N+](C)(CC)C1', '^c1ccccc1C[N+]1(CCCC1)C', '^CC[N+]1(CC)CCC(C)C(C1)C', '^C([N+](Cc1ccccc1)CCC[N+]1(C)CCCC1)(C)C', '^c1n(C)[n+](CCCC)c1C[n+]1ccn(C)c1C']\n",
      "Epoch: 1, Train Loss: 0.0, Test Loss: 2.317730988904369\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_loss_list, test_loss_list = train(n_epoch, model, train_dataloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'checkpoints/LSTM_Variant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZI0lEQVR4nO3de3Bc5Znn8d+ju2XJNlgCyzYgJ2DIhWCDsM2AgSSTxDZMCJA4mQzjmA3lYha2yNaawsxWkoLNH2xVijBMiF1k1qQgU6QSiI2pOIsnLKxJCBCZEsTc1oJhxrIM+DK+SLIkt/TsH92SWq1udUs+rVa/+n6qVH05l35fX37n9HPe88rcXQCA4ldS6AYAAKJBoANAIAh0AAgEgQ4AgSDQASAQZYX64Lq6Om9sbCzUxwNAUdq1a9dBd69Pt6xggd7Y2Kjm5uZCfTwAFCUz+7dMyyi5AEAgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQiIKNQweA4LlL3Uel4/ulY+2Jx/3S/Eukj38u8o8j0AFgPPpiUseHKWGd5vFk18htr/ivBDoATIiejkQg74ufUR9vTzwmhXXHh5L3D9+upFyqbZBmNEhnflo674vSjLmJ9xKPtQ1SeVVemk2gA5g6+vulzgNDAX1s31AZJDm0e46N3LZqplQ7Nx7WZ3wy/ljbIM2Yl3g+V6qeLZUU7tIkgQ4gDCdPDK9TH2+Pv05+r+MDqT82fDsrlWrnxMO5fqH0sauHAnpGIrBr50gV0wvSrbEg0AFMbu5S16H09enk591HRm5bUTNU6liwfHjpYyC0a86QSkonvFv5QKADKJxYT/qSR2p49/WmbGjxIK5tkE5bIJ3zFylhnXismlGQbhUKgQ4geu7xM+ZjA6HcnhLaiceugyO3LZs2dPZ81pJEOM8dXgapOVMqLZ/wbk12BDqAsek7GR/hkalOPRDWsRMjt62uGwrmeZckAjolrKtmSWYT3q0QEOgAhnQfG71OfXy/1PGRJB++XWnFUKmjYZF0/qqhOvWMeYnhenOksspC9GrKINCBqaC/Lx7EaevU+4be6+0YuW3VrKGheXMuHFmnnjFPqj6ds+pJgEAHil1vZ/r6dHJ4H/9A8r7h25WUSTVzEjfBfFI69y+Hlz4GboKpqC5MvzBmBDowWfX3x4frjVanPt4enyskVeWMoZJH3VUj69S1c6Xp9QW9CQbRI9CBQjjZnThzzlCnHjiz7j85fDsriY/wqG2QZn9carwiKajnDpVBKmsK0y8UFIEORMldOvEfaQJ63/Da9YnDI7ctnz5U6jjnspHjqmfMlaafIZXy3xbp8S8DyFWsN37reLo69cBY6+MfSLHukdtOr48H8syz4mOrk+vUgzfBzOTCIk4JgQ5kmrN6WGi3xyd1SlVamTSuumlknXpGQ/zCY1nFxPcLUw6BjrD1xaTOjzLUqZPC+2TnyG2nnT509txw0dB46uQyyLTTOKvGpEGgo3gNzlmdoU49ljmrk+vUeZ6zGsgXAh2TT+qc1al16nHNWZ18E0xh56wG8oVAx8RKO2d18pwgo8xZXXNmPJjTzVk98FgEc1YD+UKgIxqjzVk9EN7H9mWes3qgBDIF5qwG8oVAR3ax3qRwzjRn9QdSX0/KhslzVp8jnb1seJ16is5ZDeQLgT6VJc9ZnVzyGM+c1cMuKjJnNVAIBHqoRsxZneEX4uY8Z3Xy7eXMWQ1MRgR6MUo3Z3XqnCBjnbM6+c5F5qwGihKBPpmMOmd10mPGOasTZ9Bp56yeGx+ux1k1EKysgW5mZ0l6VNIcSf2SHnb3f0hZxyT9g6RVkrokrXX3V6NvbhHr7Uq5+WUcc1bXXyB9/HMpv2OROasBxOVyhh6T9N/c/VUzq5W0y8z+xd3fTFpnpaTzEj9LJW1MPIZv2JzVGerUOc9Z3TB08wtzVgMYo6yB7u77Je1PPD9uZm9JmicpOdCvk/Sou7ukl8xslpk1JLYtXqlzViePpx73nNVJv2OROasBRGhMNXQza5S0WNLLKYvmSdqb9Lot8d6wQDezdZLWSdLZZ589xqZGKOOc1SkXF9POWV09VJdON2d17cBwPS5PAJhYOaeOmdVIelLSd9w9dRKNdFfafMQb7g9LeliSmpqaRiyPxGhzVieHdqY5q2sbmLMaQFHKKdDNrFzxMP9nd/91mlXaJJ2V9Hq+pPZTb14anYek/S0Z5gLZn5izOnW4XvKc1RenXFRkzmoAYchllItJ+l+S3nL3+zOstk3S7Wb2C8Uvhh7NW/38X5+XnvhPQ69T56xO/d2KzFkNYIrI5Qz9ckl/K+nPZtaSeO/vJZ0tSe6+SdJ2xYcstio+bPHmyFs6oPFKae32ocBmzmoAkJTbKJffK32NPHkdl3RbVI0aVU19/AcAMAwDnAEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAKRNdDNbLOZfWRmuzMsv9rMjppZS+Lne9E3EwCQTVkO6/xM0o8lPTrKOi+4+7WRtAgAMC5Zz9DdfaekwxPQFgDAKYiqhn6Zmb1mZr81s09lWsnM1plZs5k1HzhwIKKPBgBI0QT6q5LOcfeLJP2jpK2ZVnT3h929yd2b6uvrI/hoAMCAUw50dz/m7h2J59sllZtZ3Sm3DAAwJqcc6GY2x8ws8XxJYp+HTnW/AICxyTrKxcwel3S1pDoza5P0fUnlkuTumyR9VdLfmVlM0glJ33B3z1uLAQBpZQ10d//rLMt/rPiwRgBAAXGnKAAEIpcbiwAgJydPnlRbW5u6u7sL3ZSiV1VVpfnz56u8vDznbQh0AJFpa2tTbW2tGhsblRgrgXFwdx06dEhtbW1asGBBzttRcgEQme7ubs2ePZswP0VmptmzZ4/5mw6BDiBShHk0xvPnSKADQCAIdADBOHLkiH7yk5+MebtVq1bpyJEjY95u7dq1euKJJ8a8Xb4Q6ACCkSnQ+/r6Rt1u+/btmjVrVp5aNXEY5QIgL+55+g292X4s0n1+cu4Mff+vMk7oqg0bNujdd9/VokWLVF5erpqaGjU0NKilpUVvvvmmvvKVr2jv3r3q7u7WHXfcoXXr1kmSGhsb1dzcrI6ODq1cuVJXXHGFXnzxRc2bN09PPfWUpk2blrVtzz77rNavX69YLKZLL71UGzduVGVlpTZs2KBt27aprKxMX/ziF/XDH/5Qv/rVr3TPPfeotLRUM2fO1M6dOyP58yHQAQTjvvvu0+7du9XS0qLnn39e11xzjXbv3j049G/z5s06/fTTdeLECV166aW68cYbNXv27GH72LNnjx5//HH99Kc/1erVq/Xkk0/qpptuGvVzu7u7tXbtWj377LNauHCh1qxZo40bN2rNmjXasmWL3n77bZnZYFnn3nvv1TPPPKN58+aNq9STCYEOIC9GO5OeKEuWLBk2jvvBBx/Uli1bJEl79+7Vnj17RgT6ggULtGjRIknSJZdcovfffz/r57zzzjtasGCBFi5cKEn61re+pYceeki33367qqqqdMstt+iaa67RtdfGf7Hb5ZdfrrVr12r16tW64YYbIuhpHDV0AMGaPn364PPnn39ev/vd7/THP/5Rr732mhYvXpx2nHdlZeXg89LSUsVisayfk2k+wrKyMr3yyiu68cYbtXXrVq1YsUKStGnTJv3gBz/Q3r17tWjRIh06FM0EtZyhAwhGbW2tjh8/nnbZ0aNHddppp6m6ulpvv/22Xnrppcg+94ILLtD777+v1tZWnXvuuXrsscd01VVXqaOjQ11dXVq1apWWLVumc889V5L07rvvaunSpVq6dKmefvpp7d27d8Q3hfEg0AEEY/bs2br88sv16U9/WtOmTdOZZ545uGzFihXatGmTPvOZz+j888/XsmXLIvvcqqoqPfLII/ra1742eFH01ltv1eHDh3Xdddepu7tb7q4f/ehHkqQ777xTe/bskbvr85//vC666KJI2mGFmrq8qanJm5ubC/LZAPLjrbfe0ic+8YlCNyMY6f48zWyXuzelW58aOgAEgpILAGRx22236Q9/+MOw9+644w7dfPPNBWpRegQ6AGTx0EMPFboJOaHkAgCBINABIBAEOgAEgkAHgEAQ6ACCMd750CXpgQceUFdX16jrNDY26uDBg+Pa/0Qg0AEEI9+BPtkxbBFAfvx2g/TBn6Pd55wLpZX3ZVycPB/6F77wBZ1xxhn65S9/qZ6eHl1//fW655571NnZqdWrV6utrU19fX367ne/qw8//FDt7e367Gc/q7q6Oj333HNZm3L//fdr8+bNkqRbbrlF3/nOd9Lu++tf/3raOdHzgUAHEIzk+dB37NihJ554Qq+88orcXV/+8pe1c+dOHThwQHPnztVvfvMbSfFJu2bOnKn7779fzz33nOrq6rJ+zq5du/TII4/o5Zdflrtr6dKluuqqq/Tee++N2Pfhw4fTzomeDwQ6gPwY5Ux6IuzYsUM7duzQ4sWLJUkdHR3as2ePli9frvXr1+uuu+7Stddeq+XLl49537///e91/fXXD07Pe8MNN+iFF17QihUrRuw7FoulnRM9H6ihAwiSu+vuu+9WS0uLWlpa1Nraqm9/+9tauHChdu3apQsvvFB333237r333nHtO510+840J3o+EOgAgpE8H/qXvvQlbd68WR0dHZKkffv26aOPPlJ7e7uqq6t10003af369Xr11VdHbJvNlVdeqa1bt6qrq0udnZ3asmWLli9fnnbfHR0dOnr0qFatWqUHHnhALS0teem7RMkFQECS50NfuXKlvvnNb+qyyy6TJNXU1OjnP/+5Wltbdeedd6qkpETl5eXauHGjJGndunVauXKlGhoasl4Uvfjii7V27VotWbJEUvyi6OLFi/XMM8+M2Pfx48fTzomeD8yHDiAyzIceLeZDB4ApipILAKRYunSpenp6hr332GOP6cILLyxQi3JDoAOIlLvLzArdjFPy8ssvF7oJGUfSjIaSC4DIVFVV6dChQ+MKIwxxdx06dEhVVVVj2o4zdACRmT9/vtra2nTgwIFCN6XoVVVVaf78+WPahkAHEJny8nItWLCg0M2YsrKWXMxss5l9ZGa7Myw3M3vQzFrN7HUzuzj6ZgIAssmlhv4zSaPdq7pS0nmJn3WSNp56swAAY5U10N19p6TDo6xynaRHPe4lSbPMrCGqBgIAchPFKJd5kvYmvW5LvDeCma0zs2Yza+aiCQBEK4pATzfgNO2YJXd/2N2b3L2pvr4+go8GAAyIItDbJJ2V9Hq+pPYI9gsAGIMoAn2bpDWJ0S7LJB119/0R7BcAMAZZx6Gb2eOSrpZUZ2Ztkr4vqVyS3H2TpO2SVklqldQl6eZ8NRYAkFnWQHf3v86y3CXdFlmLAADjwlwuABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4Agcgp0M1shZm9Y2atZrYhzfKrzeyombUkfr4XfVMBAKMpy7aCmZVKekjSFyS1SfqTmW1z9zdTVn3B3a/NQxsBADnI5Qx9iaRWd3/P3Xsl/ULSdfltFgBgrHIJ9HmS9ia9bku8l+oyM3vNzH5rZp9KtyMzW2dmzWbWfODAgXE0FwCQSS6Bbmne85TXr0o6x90vkvSPkram25G7P+zuTe7eVF9fP6aGAgBGl0ugt0k6K+n1fEntySu4+zF370g83y6p3MzqImslACCrXAL9T5LOM7MFZlYh6RuStiWvYGZzzMwSz5ck9nso6sYCADLLOsrF3WNmdrukZySVStrs7m+Y2a2J5ZskfVXS35lZTNIJSd9w99SyDAAgj6xQudvU1OTNzc0F+WwAKFZmtsvdm9It405RAAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIRNZb/yeb9w926oU9B1RdUabplWWaXlmaeF6q6Yn3qitKVVlWosT0MgAwJRRdoL++76i++9QbWdcrKzFVV5QOBnxNZdlg8A8eDCpKVZ14HHZwGDhApGxbUcYXGgCTV9EF+opPzdGf/vtfqrMnps7emLp6++LPe/rir3ti6ky815X02NETU1dvTO1HTqqzN75+V2L7XJWXmqoryhIBn3IwGPG6TNWVSQeSxPKapINGdWWpyks5SACIRtEFekVZieprK1VfWxnJ/vr7XV0n+4YdCAYPBr0xdfUMHQw6e+PrdSQOBgPrH+7sUldv/L2Onpi6T/aPqT/TK5LKRgMHg6RvDfHXw781TE9XcqosU3V5qco4SABTUtEFetRKSkw1lfGz7qj09fvg2X9HT/ygEP82kTgY9CQdHHqHlid/qzjY0ZM4kMRf98RyP0hUlpWkHAyGf2uYPsaSU3VFmUpLuB4BTHZTPtDzobTEVFtVrtqqcp0Z0T5jff3xg0BSuWj4wWKo9DS4LKnkdLw7pg+PdQ8u7+zpU29f7geJaeWlg4E/eF0hh5JT8oXq6UnLp5WXqoSDBBApAr1IlJWWaOa0Es2cVh7ZPntj/TrRO/TtYOBbw2DpKUvJ6eiJk9p/5MTgtp09McX6c59ff+Dsv6Zy9JLT8GsQZRkvdk8rL2VkE6Y0An0KqygrUUVZiWZWR3uQyHjBOk3JKfkCdWdPnw539mrv4a6hbXv71JfjQcJMox8Mkr415HLBuqayjOGvKCoEOiIVP0hU6LTpFZHsz93VE+tPCvjkA0DS85SSU/IF7oMdveo83BUvPyUOLrl+kShJHCSmVyYdDFJKToPfMpJeJx9QUr9VcJBAvhDomNTMTFXlpaoqL9XpER4kuk/2D5aUspWc0i3/4Fh30jeQ+Hu5OpV7JDKVnLhHAhKBjinIzDStolTTKkqlmmj22d/v6o71xctL3COBAiHQgQiUlFhiBFBZwe6RSFdyytc9EjXDSlDcIzFZEOjAJFXIeyTSlZy4R2LyI9CBKaQQ90gkj2LK9z0S6UtMU+ceCQIdwCmZyHskOkYpOUV9j8TgwaCI7pEg0AFMOhNxj0S2aTnyeY/E3yw9W7cs/1hkfRtAoAOYEibTPRJ1NdFcOE9FoAPAOOTjHolTxRgiAAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCDMPff5DSL9YLMDkv5tnJvXSToYYXOKAX2eGujz1HAqfT7H3evTLShYoJ8KM2t296ZCt2Mi0eepgT5PDfnqMyUXAAgEgQ4AgSjWQH+40A0oAPo8NdDnqSEvfS7KGjoAYKRiPUMHAKQg0AEgEJM60M1shZm9Y2atZrYhzXIzswcTy183s4sL0c4o5dDnv0n09XUze9HMLipEO6OUrc9J611qZn1m9tWJbF8+5NJnM7vazFrM7A0z+78T3cao5fBve6aZPW1mryX6fHMh2hkVM9tsZh+Z2e4My6PPL3eflD+SSiW9K+ljkiokvSbpkynrrJL0W0kmaZmklwvd7gno819IOi3xfOVU6HPSev9H0nZJXy10uyfg73mWpDclnZ14fUah2z0Bff57Sf8z8bxe0mFJFYVu+yn0+UpJF0vanWF55Pk1mc/Ql0hqdff33L1X0i8kXZeyznWSHvW4lyTNMrOGiW5ohLL22d1fdPf/SLx8SdL8CW5j1HL5e5ak/yLpSUkfTWTj8iSXPn9T0q/d/d8lyd2Lvd+59Nkl1ZqZSapRPNBjE9vM6Lj7TsX7kEnk+TWZA32epL1Jr9sS7411nWIy1v58W/EjfDHL2mczmyfpekmbJrBd+ZTL3/NCSaeZ2fNmtsvM1kxY6/Ijlz7/WNInJLVL+rOkO9y9f2KaVxCR59dk/iXRlua91DGWuaxTTHLuj5l9VvFAvyKvLcq/XPr8gKS73L0vfvJW9HLpc5mkSyR9XtI0SX80s5fc/f/lu3F5kkufvySpRdLnJH1c0r+Y2QvufizPbSuUyPNrMgd6m6Szkl7PV/zIPdZ1iklO/TGzz0j6J0kr3f3QBLUtX3Lpc5OkXyTCvE7SKjOLufvWCWlh9HL9t33Q3TsldZrZTkkXSSrWQM+lzzdLus/jBeZWM/tXSRdIemVimjjhIs+vyVxy+ZOk88xsgZlVSPqGpG0p62yTtCZxtXiZpKPuvn+iGxqhrH02s7Ml/VrS3xbx2VqyrH129wXu3ujujZKekPSfizjMpdz+bT8labmZlZlZtaSlkt6a4HZGKZc+/7vi30hkZmdKOl/SexPayokVeX5N2jN0d4+Z2e2SnlH8Cvlmd3/DzG5NLN+k+IiHVZJaJXUpfoQvWjn2+XuSZkv6SeKMNeZFPFNdjn0OSi59dve3zOx/S3pdUr+kf3L3tMPfikGOf8//Q9LPzOzPipcj7nL3op1W18wel3S1pDoza5P0fUnlUv7yi1v/ASAQk7nkAgAYAwIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABOL/A6PnAcHjZhMXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training and testing loss\n",
    "plot_loss(train_loss_list, test_loss_list, 'LSTM_Variant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
