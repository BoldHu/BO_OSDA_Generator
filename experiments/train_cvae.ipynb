{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CVAE Model for Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hudongcheng/Desktop/bo_osda_generator\n"
     ]
    }
   ],
   "source": [
    "# change working path to the current file\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from models.cvae import *\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from utils.plot_figures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "log_dir = './logs/'\n",
    "save_best_weight_path = './checkpoints/'\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the charset(inculde begin end and pad char) achieved from dataset : ?P25$]FO-S.Hc=71(ln63NC4[+)^@\n",
      "the total num of charset is : 29\n"
     ]
    }
   ],
   "source": [
    "# read the data and convert to the format we need\n",
    "train_smiles = read_strings('./data/train_smiles.csv', idx=False)\n",
    "train_zeo = read_vec('./data/train_zeo.csv', idx=False)\n",
    "train_syn = read_vec('./data/train_syn.csv', idx=False)\n",
    "train_codes = read_strings('./data/train_codes.csv', idx=False)\n",
    "test_smiles = read_strings('./data/test_smiles.csv', idx=False)\n",
    "test_zeo = read_vec('./data/test_zeo.csv', idx=False)\n",
    "test_syn = read_vec('./data/test_syn.csv', idx=False)\n",
    "test_codes = read_strings('./data/test_codes.csv', idx=False)\n",
    "\n",
    "charset = '?P25$]FO-S.Hc=71(ln63NC4[+)^@'\n",
    "charlen = len(charset)\n",
    "print('the charset(inculde begin end and pad char) achieved from dataset :', charset)\n",
    "print('the total num of charset is :', charlen)\n",
    "# create the char to index and index to char dictionary\n",
    "char_to_index = dict((c, i) for i, c in enumerate(charset))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(charset))\n",
    "char_list = [k for k, v in char_to_index.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 512\n",
    "epoch = 10\n",
    "seqlen = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_smiles, tgt_smiles = smiles_padding(train_smiles)\n",
    "tgt_seq = smiles_to_sequence(tgt_smiles, char_to_index)\n",
    "tgt_seq = torch.cat([torch.unsqueeze(seq, 0) for seq in tgt_seq]).long()\n",
    "src_smiles_test, tgt_smiles_test = smiles_padding(test_smiles)\n",
    "tgt_seq_test = smiles_to_sequence(tgt_smiles_test, char_to_index)\n",
    "tgt_seq_test = torch.cat([torch.unsqueeze(seq, 0) for seq in tgt_seq_test]).long()\n",
    "# create the dataset and dataloader\n",
    "train_dataset = SeqDataset(train_zeo, train_syn, tgt_seq)\n",
    "test_dataset = SeqDataset(test_zeo, test_syn, tgt_seq_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1.14M\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have:\n",
    "params = {\n",
    "    'NCHARS': 29,\n",
    "    'MAX_LEN': 127,\n",
    "    'COND_DIM': 24,\n",
    "    'hidden_dim': 256,\n",
    "    'conv_depth': 3,\n",
    "    'conv_dim_depth': 64,\n",
    "    'conv_dim_width': 3,\n",
    "    'conv_d_growth_factor': 2,\n",
    "    'conv_w_growth_factor': 1,\n",
    "    'middle_layer': 1,\n",
    "    'activation': 'tanh',\n",
    "    'batchnorm_conv': True,\n",
    "    'batchnorm_mid': True,\n",
    "    'dropout_rate_mid': 0.2,\n",
    "    'gru_depth': 2,\n",
    "    'recurrent_dim': 256,\n",
    "    # ...\n",
    "}\n",
    "\n",
    "encoder = ConditionalEncoder(\n",
    "    input_channels=params['NCHARS'],\n",
    "    max_len=params['MAX_LEN'],\n",
    "    cond_dim=params['COND_DIM'],\n",
    "    hidden_dim=params['hidden_dim'],\n",
    "    conv_depth=params['conv_depth'],\n",
    "    conv_dim_depth=params['conv_dim_depth'],\n",
    "    conv_dim_width=params['conv_dim_width'],\n",
    "    conv_d_growth_factor=params['conv_d_growth_factor'],\n",
    "    conv_w_growth_factor=params['conv_w_growth_factor'],\n",
    "    middle_layer=params['middle_layer'],\n",
    "    activation=params['activation'],\n",
    "    batchnorm_conv=params['batchnorm_conv'],\n",
    "    batchnorm_mid=params['batchnorm_mid'],\n",
    "    dropout_rate_mid=params['dropout_rate_mid']\n",
    ").to(device)\n",
    "\n",
    "decoder = ConditionalDecoder(\n",
    "    hidden_dim=params['hidden_dim'],\n",
    "    cond_dim=params['COND_DIM'],\n",
    "    n_chars=params['NCHARS'],\n",
    "    max_len=params['MAX_LEN'],\n",
    "    gru_depth=params['gru_depth'],\n",
    "    recurrent_dim=params['recurrent_dim'],\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "model = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# loss\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, dataloader, loss_func, optim, device, kl_weight=0.001, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: your Conditional VAE\n",
    "        dataloader: yields (zeo, syn, tgt) each step\n",
    "        loss_func: typically nn.CrossEntropyLoss(ignore_index=pad_idx) or similar\n",
    "        optim: torch optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        kl_weight: scaling factor for the KL loss term\n",
    "        pad_idx: optional index for <pad>, used in ignoring pad in cross-entropy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (zeo, syn, tgt) in enumerate(tqdm(dataloader)):\n",
    "        # Move data to device\n",
    "        zeo = zeo.to(device)\n",
    "        syn = syn.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # Concatenate to form the full condition\n",
    "        # shape: (batch_size, cond_dim1 + cond_dim2)\n",
    "        condition = torch.cat([zeo, syn], dim=-1)\n",
    "        # tgt: shape (B, seq_len, n_chars) if one-hot approach\n",
    "        tgt = F.one_hot(tgt, num_classes=params['NCHARS']).float()\n",
    "        tgt_input = tgt[:, :-1].contiguous() # input to the decoder\n",
    "        tgt_target = tgt[:, 1:].contiguous() # target output from the decoder\n",
    "\n",
    "        # ==========================\n",
    "        # 1) Forward pass\n",
    "        # ==========================\n",
    "        # We do teacher forcing by passing 'teacher_force_inputs=tgt'\n",
    "        # 'tgt' can be one-hot or integer-based, depending on your model design.\n",
    "        logits, z_mean, z_log_var = model(\n",
    "            x_smi=tgt_input,         # SMILES input\n",
    "            x_cond=condition,  # condition\n",
    "            teacher_force_inputs=tgt_input\n",
    "        )\n",
    "        # logits: shape (B, seq_len, n_chars) if one-hot approach\n",
    "        # or (B, seq_len, vocab_size) if integer token approach\n",
    "\n",
    "        # ==========================\n",
    "        # 2) Compute reconstruction loss\n",
    "        # ==========================\n",
    "        # If 'tgt' is one-hot, we might do:\n",
    "        #   target_ids = tgt.argmax(dim=-1)  # shape (B, seq_len)\n",
    "        # If 'tgt' is already integer-based, you can use 'tgt' directly.\n",
    "        \n",
    "        # Example: if 'tgt' is one-hot:\n",
    "        target_ids = tgt_target.argmax(dim=-1)  # (B, seq_len)\n",
    "\n",
    "        # Cross-entropy expects (B, vocab_size, seq_len), so we permute\n",
    "        # Or you can define your model to output (B, vocab_size, seq_len) directly.\n",
    "        # Just adapt as needed:\n",
    "        logits_permuted = logits.permute(0, 2, 1)  # (B, n_chars, seq_len)\n",
    "\n",
    "        recon_loss = loss_func(logits_permuted, target_ids)\n",
    "\n",
    "        # ==========================\n",
    "        # 3) Compute KL divergence\n",
    "        # ==========================\n",
    "        # KL = -0.5 * sum(1 + log_var - mean^2 - exp(log_var)) \n",
    "        # We'll average by batch size\n",
    "        kl_loss = -0.5 * torch.sum(\n",
    "            1 + z_log_var - z_mean.pow(2) - z_log_var.exp()\n",
    "        )\n",
    "        kl_loss = kl_loss / tgt.size(0)\n",
    "\n",
    "        # ==========================\n",
    "        # 4) Total loss\n",
    "        # ==========================\n",
    "        loss = recon_loss + kl_weight * kl_loss\n",
    "\n",
    "        # ==========================\n",
    "        # 5) Backprop & update\n",
    "        # ==========================\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # ==========================\n",
    "        # 6) Compute accuracy (optional)\n",
    "        # ==========================\n",
    "        # We can do a simple token-level accuracy for demonstration:\n",
    "        preds = torch.argmax(logits, dim=-1)  # shape (B, seq_len)\n",
    "        correct = (preds == target_ids).float()  # (B, seq_len)\n",
    "        # If you want to exclude pads from accuracy:\n",
    "        if pad_idx is not None:\n",
    "            # create a mask for non-pad positions\n",
    "            mask = (target_ids != pad_idx).float()\n",
    "            accuracy = (correct * mask).sum() / mask.sum()\n",
    "        else:\n",
    "            accuracy = correct.mean()\n",
    "\n",
    "        # ==========================\n",
    "        # 7) Track stats\n",
    "        # ==========================\n",
    "        batch_size = tgt.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_acc += accuracy.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(model, dataloader, loss_func, device, pad_idx=0):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (zeo, syn, tgt) in enumerate(tqdm(dataloader)):\n",
    "            # Move data to device\n",
    "            zeo = zeo.to(device)\n",
    "            syn = syn.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            # Concatenate to form the full condition\n",
    "            condition = torch.cat([zeo, syn], dim=-1)\n",
    "            tgt = F.one_hot(tgt, num_classes=params['NCHARS']).float()\n",
    "            tgt_input = tgt[:, :-1].contiguous()  # exclude the last token for prediction\n",
    "            tgt_label = tgt[:, 1:].contiguous()   # exclude the first token for prediction\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, z_mean, z_log_var = model(\n",
    "                x_smi=tgt_input,         # SMILES input\n",
    "                x_cond=condition,  # condition\n",
    "                teacher_force_inputs=None\n",
    "            )\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            target_ids = tgt_label.argmax(dim=-1)\n",
    "            logits_permuted = logits.permute(0, 2, 1)\n",
    "            recon_loss = loss_func(logits_permuted, target_ids)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct = (preds == target_ids).float()\n",
    "            if pad_idx is not None:\n",
    "                mask = (target_ids != pad_idx).float()\n",
    "                accuracy = (correct * mask).sum() / mask.sum()\n",
    "            else:\n",
    "                accuracy = correct.mean()\n",
    "            \n",
    "            # Track stats\n",
    "            batch_size = tgt.size(0)\n",
    "            total_loss += recon_loss.item() * batch_size\n",
    "            total_acc += accuracy.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/284 [00:00<?, ?it/s]/home/hudongcheng/miniconda3/envs/zeosyn/lib/python3.7/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "100%|██████████| 284/284 [00:05<00:00, 51.75it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 93.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 0.5026, train acc 0.4687, test loss 6.7835, test acc 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.23it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 112.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.2266, train acc 0.7044, test loss 7.8421, test acc 0.1148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.32it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 108.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, train loss 0.1817, train acc 0.7675, test loss 5.9313, test acc 0.1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.72it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 112.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, train loss 0.1562, train acc 0.8052, test loss 6.8438, test acc 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.22it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 104.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, train loss 0.1375, train acc 0.8324, test loss 7.8054, test acc 0.1230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.26it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 112.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, train loss 0.1228, train acc 0.8537, test loss 8.8309, test acc 0.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 60.82it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 112.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, train loss 0.1102, train acc 0.8712, test loss 9.7755, test acc 0.1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.15it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 107.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, train loss 0.1002, train acc 0.8850, test loss 9.8139, test acc 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.14it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 107.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, train loss 0.0917, train acc 0.8970, test loss 10.3311, test acc 0.1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 61.39it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 112.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, train loss 0.0848, train acc 0.9067, test loss 11.1185, test acc 0.1301\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for i in range(epoch):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_func, optim, device)\n",
    "    test_loss, test_acc = test(model, test_dataloader, loss_func, device)\n",
    "    print('epoch %d, train loss %.4f, train acc %.4f, test loss %.4f, test acc %.4f' % (i, train_loss, train_acc, test_loss, test_acc))\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_acc_history.append(test_acc)\n",
    "    if i == 0:\n",
    "        best_acc = test_acc\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), os.path.join(save_best_weight_path, 'best_ddc_model.pth'))\n",
    "    torch.save(model.state_dict(), os.path.join(save_best_weight_path, 'last_ddc_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_cvae(model, condition, idx2char, device='cuda', max_len=100):\n",
    "    \"\"\"\n",
    "    Generates SMILES strings from a trained CVAE model, given an already-merged synthesis condition.\n",
    "\n",
    "    Args:\n",
    "        model:       The trained ConditionalVAE model (must implement model.generate).\n",
    "        condition:   (B, cond_dim) - the already-merged condition vector (e.g., torch.cat([zeo, syn], dim=-1)).\n",
    "        idx2char:    A list or dict mapping token indices -> SMILES characters.\n",
    "        device:      'cuda' or 'cpu'.\n",
    "        max_len:     Maximum length of generated SMILES.\n",
    "\n",
    "    Returns:\n",
    "        smiles_list: A list of generated SMILES strings (length = batch size).\n",
    "                     Each string excludes '^', ignores '?', and is cut after '$'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    smiles_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Move condition to device\n",
    "        condition = condition.to(device)\n",
    "\n",
    "        # Generate token IDs: shape = (B, max_len)\n",
    "        preds = model.generate(condition, max_len=max_len, device=device)\n",
    "\n",
    "        # Convert each sequence of token IDs into a SMILES string\n",
    "        for seq in preds:\n",
    "            seq = seq.cpu().numpy()  # shape (max_len,)\n",
    "            tokens = []\n",
    "            for token_id in seq:\n",
    "                char = idx2char[token_id]\n",
    "\n",
    "                # 1) Skip '^' (start char)\n",
    "                if char == '^':\n",
    "                    continue\n",
    "\n",
    "                # 2) Skip '?' (pad char)\n",
    "                if char == '?':\n",
    "                    continue\n",
    "\n",
    "                # 3) Break on '$' (end char)\n",
    "                if char == '$':\n",
    "                    break\n",
    "\n",
    "                tokens.append(char)\n",
    "\n",
    "            # Join remaining tokens into a SMILES\n",
    "            smiles_str = \"\".join(tokens)\n",
    "            smiles_list.append(smiles_str)\n",
    "\n",
    "    return smiles_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target smiles:\n",
      "[['n1(C)c(C)[n+](cc1)Cc1ccccc1C[n+]1ccn(C)c1C']\n",
      " ['Cn1cc[n+](Cc2ccccc2C[n+]2ccn(C)c2C)c1C']\n",
      " ['Cc1n(C)cc[n+]1Cc1c(cccc1)C[n+]1ccn(c1C)C']\n",
      " ['c1ccc(c(c1)C[n+]1c(C)n(C)cc1)C[n+]1ccn(c1C)C']\n",
      " ['[n+]1(c(n(C)cc1)C)Cc1c(C[n+]2ccn(c2C)C)cccc1']\n",
      " ['Cc1n(C)cc[n+]1Cc1ccccc1C[n+]1c(n(cc1)C)C']\n",
      " ['Cn1c(C)[n+](cc1)Cc1ccccc1C[n+]1ccn(c1C)C']\n",
      " ['c1c[n+](Cc2ccccc2C[n+]2c(C)n(cc2)C)c(C)n1C']\n",
      " ['n1(cc[n+](c1C)Cc1ccccc1C[n+]1ccn(C)c1C)C']\n",
      " ['[n+]1(Cc2c(C[n+]3c(n(cc3)C)C)cccc2)c(C)n(C)cc1']]\n",
      "generated smiles:\n",
      "['cccnnnnnnnncccccccccc11111ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'cccccCCCCCCCCCCCCCCNNNNN[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[', 'CNNccccccccccccccccnnnnnnncccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'ccnnncccccccccccccccccccccccccccccccccccccccccccccccccccccc11cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CcCCCCCCCC[[NNNN))))))))))))))))))))))))))))))))))))))))))))))))))))))))[[[[[[[[[[[[[[[[[))))))))))))))))))))))22222222222222', 'CONcccccccccccccccccccccccccccccccccccNNN)))))))))))))222222222222222222222222222222222222222222222222222222222222222', 'ccCCCCccccCCNNN))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))', 'NNNNNNNNNNNccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc1111ccccccccccccccccccccccccc', 'c111111111[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[))))))))))))))))))))))))))))))))))))))))))))))))))))))))))', 'CCCCcccccnnnnnnnnnnnn[[[[222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222']\n"
     ]
    }
   ],
   "source": [
    "# test the generate function\n",
    "# check the first 10 samples\n",
    "train_zeo = train_zeo[:10].astype(np.float32)\n",
    "train_syn = train_syn[:10].astype(np.float32)\n",
    "zeo = torch.tensor(train_zeo, dtype=torch.float32).to(device)\n",
    "syn = torch.tensor(train_syn, dtype=torch.float32).to(device)\n",
    "target_smi = train_smiles[:10]\n",
    "condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "generated_smiles = generate_cvae(model=model, condition=condition_synthesis, idx2char=index_to_char, device=device, max_len=127)\n",
    "print('target smiles:')\n",
    "print(target_smi)\n",
    "print('generated smiles:')\n",
    "print(generated_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhFElEQVR4nO3deXxU1d3H8c+ZyUYWQBJEFjFBBbUuIGFRREQUQagKKm6o8NRiq7boo7Zo69qn1bYWl4r4oII+4ApK1WoLiiAoyiaoCFp2E0AIIEuAAEnO88eZJJMFCJlJ7tzk+37JKzN37vLLCN85c+695xhrLSIi4j8BrwsQEZGaUYCLiPiUAlxExKcU4CIiPqUAFxHxqbi6PFhGRobNzMysy0OKiPjeokWLtlhrm1dcXqcBnpmZycKFC+vykCIivmeMWVfVcnWhiIj4lAJcRMSnFOAiIj5Vp33gVTlw4AC5ubkUFBR4XYrvJSUl0aZNG+Lj470uRUTqgOcBnpubS1paGpmZmRhjvC7Ht6y1bN26ldzcXLKysrwuR0TqgOddKAUFBaSnpyu8I2SMIT09Xd9kRBoQzwMcUHhHid5HkYYlJgJcRKTe2rIS/n0v7NkW9V173gcuIlLvFBXCimkw/zlYPRMCcZDVEzr0j+phGnwLfPv27TzzzDNHvN3FF1/M9u3bj3i7YcOGMWXKlCPeTkR8ID8PZj8GT3WE166FvO+g9+/gjmVRD29QC7w0wG+55ZZyy4uKiggGgwfd7v3336/t0kTED6yF3AWutb3sH1C0H7LOhYv+BB0uhmDtxWxMBfhD737Dsg07o7rPU1o15oGf/uSgr48aNYpVq1bRsWNH4uPjSU1NpWXLlixZsoRly5Zx2WWXkZOTQ0FBASNHjmTEiBFA2bgu+fn59O/fn3POOYe5c+fSunVr3n77bRo1anTY2mbMmMFdd91FYWEhXbp0YezYsSQmJjJq1Cjeeecd4uLi6Nu3L4899hiTJ0/moYceIhgM0qRJE2bPnh2190hEamD/Hvh6Mix4Hn74ChLSoPMw6HITNO9QJyXEVIB74dFHH2Xp0qUsWbKEWbNmMWDAAJYuXVp6LfX48eNp1qwZe/fupUuXLlx++eWkp6eX28eKFSt49dVXee655xgyZAhvvvkmQ4cOPeRxCwoKGDZsGDNmzKB9+/bccMMNjB07lhtuuIGpU6fy7bffYowp7aZ5+OGHmTZtGq1bt65R142IRMnWVbDgBVgyCQp2wNGnwIDRcPpVkJhap6XEVIAfqqVcV7p27VruRpinnnqKqVOnApCTk8OKFSsqBXhWVhYdO3YEoHPnzqxdu/awx/nuu+/Iysqiffv2ANx4442MGTOG2267jaSkJG666SYGDBjAwIEDAejRowfDhg1jyJAhDB48OAq/qYhUW3ER/GcaLHgOVn3kTkqefIlrbR93Nnh0CW9MBXgsSElJKX08a9YsPvzwQz777DOSk5M577zzqrxRJjExsfRxMBhk7969hz2OtbbK5XFxccyfP58ZM2bw2muv8fTTT/PRRx/x7LPPMm/ePN577z06duzIkiVLKn2QiEiU7d4CX/wfLJwAO76HtJZw3r3Q+UZIO8br6hTgaWlp7Nq1q8rXduzYwVFHHUVycjLffvstn3/+edSOe9JJJ7F27VpWrlzJCSecwMSJE+nVqxf5+fns2bOHiy++mO7du3PCCScAsGrVKrp160a3bt149913ycnJUYCL1AZrIXeha21/M9WdlMzsCX3/ACcNgGDsjDXU4AM8PT2dHj16cOqpp9KoUSNatGhR+lq/fv149tlnOf300+nQoQPdu3eP2nGTkpKYMGECV155ZelJzF/84hds27aNSy+9lIKCAqy1PP744wDcfffdrFixAmstffr04YwzzohaLSKCOym59E0X3Bu/dCclz7zRdZMcfZLX1VXJHOyrfG3Izs62FWfkWb58OSeffHKd1VDf6f0UOUJbV8HC8bB4EhRsh+YnQ9ebQicl07yuDgBjzCJrbXbF5Q2+BS4iDVBxEaz4wLW2V37oTkqeNBC6/hyO6+HZSckjpQCvJbfeeiuffvppuWUjR45k+PDhHlUkIuzeCosnwsIXYPv3kHoMnHeP6ypp3NLr6o6YAryWjBkzxusSRKRE7iLX2l76FhTtg+POgQsfdq3uGDopeaQU4CJSP+3cCCumw6IJsGExJKTCmddD9s+gxSleVxcVCnARqR8O7IV1c92NNqs+gs3L3PKMDnDxY+6kZFJjb2uMMgW4iPiTtbDpm7LAXjfXdY8EE6DtWa6LpF1vOOY035yUPFIKcBHxj/zNsGqmC+zVMyF/k1ve/GR3vfbx57tb2xOSva2zjhw2wI0x44GBwGZr7amhZc2A14FMYC0wxFr7Y+2VWXu2b9/OK6+8Umk42ep44oknGDFiBMnJB//LUjJqYUZGRiRlijRMBwog5/OyVvYPX7vljZrB8b3h+D7uZ+NW3tbpkepM6PAi0K/CslHADGvticCM0HNfqumEDuACfM+ePVGuSKQBsxY2L4fPxsCky+HPmfB/l8Jnz0BSU+hzP4yYBXevgivGQ6frGmx4QzVa4Nba2caYzAqLLwXOCz1+CZgF/Dbiav41quwTNlqOOQ36P3rQl8PHA7/wwgs5+uijeeONN9i3bx+DBg3ioYceYvfu3QwZMoTc3FyKioq477772LRpExs2bKB3795kZGQwc+bMw5YyevRoxo8fD8BNN93E7bffXuW+r7rqqirHBBepl3Zvdd0hJV0juza45eknukGjjj/f3VxTx0O1+kFN+8BbWGs3AlhrNxpjjj7YisaYEcAIgLZt29bwcLUnfDzw6dOnM2XKFObPn4+1lksuuYTZs2eTl5dHq1ateO+99wA3yFWTJk0YPXo0M2fOrFb3yKJFi5gwYQLz5s3DWku3bt3o1asXq1evrrTvbdu2VTkmuEi9ULgfcuaVdYts/BKwroXd7jwX2Mf3hqaxlxexptZPYlprxwHjwI2FcsiVD9FSrgvTp09n+vTpdOrUCYD8/HxWrFhBz549ueuuu/jtb3/LwIED6dmz5xHv+5NPPmHQoEGlw9UOHjyYOXPm0K9fv0r7LiwsrHJMcBFfsha2riwL7DVz4MBuMEE4tiv0vtf1ZbfqCIGDT2MoldU0wDcZY1qGWt8tgc3RLMor1lruuecebr755kqvLVq0iPfff5977rmHvn37cv/99x/xvqvSvn37Kvdd1ZjgIjGnuBiKD7ghV4sOQHGh+1m0z7WsV33kukZ25Lj1m7WDjte4VnZmz3p3XXZdq2mAvwPcCDwa+vl21CqqY+HjgV900UXcd999XHfddaSmprJ+/Xri4+MpLCykWbNmDB06lNTUVF588cVy21anC+Xcc89l2LBhjBo1CmstU6dOZeLEiWzYsKHSvg82JrjIYa2b6ybYLToQCtQDFYJ1f9njktcqrVcxjCtuE1qn+ADY4kPXk9jYTfDb87/dNdnNsg69vhyR6lxG+CruhGWGMSYXeAAX3G8YY34GfA9cWZtF1qbw8cD79+/Ptddey1lnnQVAamoqkyZNYuXKldx9990EAgHi4+MZO3YsACNGjKB///60bNnysCcxzzzzTIYNG0bXrl0BdxKzU6dOTJs2rdK+d+3aVeWY4CIHtWYOfPxnWDun/HITdGN9BBPciHvBeAjEu5nSgwlljwOhdYLxEJ8c2qZk3cNtE1e2Xvg2zdpB6+xanZW9odN44PWM3s8GZu0nMOtRF9ypx8A5d8AZV7sQDsRBoDpXCkus03jgIvVJxeDu92d3yV18I68rkzqkAI+Sbt26sW/fvnLLJk6cyGmnneZRRVIvlQvuFgruBi4mAtxai/H5YDPz5s3zuoSDXuki9UCl4H4UOg9TcDdwngd4UlISW7duJT093fch7iVrLVu3biUpKcnrUiSa1n4Ksx5RcEuVPA/wNm3akJubS15entel+F5SUhJt2rTxugyJBgW3VIPnAR4fH09Wlq4NFQEU3HJEPA9wEUHBLTWiABfx0tpP4eNHYc1sBbccMQW4iBfWzXUt7pLgvugRyB6u4JYjogAXqUsKbokiBbhIXQgP7pSjFdwSFQpwkdq0bq67AWfNx2XB3XlYg5l0V2qXAlykNqz7LNTiVnBL7VGAi0STglvqkAJcJBpyFsBHfwgL7j9B5+EKbqlVCnCRSGxZCTMehOXvQkpzBbfUKQW4SE3kb3YnJxe96K4k6f076H4LJKZ6XZk0IApwkSOxbxfMfRrm/t1N3Jv9X9Drt5Da3OvKpAFSgItUR9EB19r++M+wOw9OuQz63A/px3tdmTRgCnCRQ7EWlr0NMx6GbavguHPgmtehTWevKxNRgIsc1NpP4YP7Yf1CaH4yXPsGnNgXNPGIxAgFuEhFm5fDhw/Cf/4Naa3g0jFwxjUQCHpdmUg5CnCREjvWw6w/wZJXICENLngQuv1C45VIzFKAi+zdDp8+AZ+PBVvsLgfseSckN/O6MpFDUoBLw1W4DxY8D7P/Cnt/hNOGwPm/h6OO87oykWpRgEvDU1wMS6e4W9+3fw/tesOFD0HLM7yuTOSIKMClYVn1EXzwAPzwFRxzOlz/JBx/vtdVidRIRAFujLkDuAmwwNfAcGttQTQKE4mqjV+64F49E5q2hcHPwalXQCDgdWUiNVbjADfGtAZ+DZxird1rjHkDuBp4MUq1iUTux3Uw84/w1evQ6Cg32FSXmyAu0evKRCIWaRdKHNDIGHMASAY2RF6SSBTs2QZz/gbzx4EJwDl3QI/boVFTrysTiZoaB7i1dr0x5jHge2AvMN1aO73iesaYEcAIgLZt29b0cCLVc2AvzHsW5jwO+3fBGddC73uhSWuvKxOJuhp3ABpjjgIuBbKAVkCKMWZoxfWsteOstdnW2uzmzTVim9SS4iJYPAn+3tndRXncWfCLT+GyMQpvqbci6UK5AFhjrc0DMMa8BZwNTIpGYTHnx3Vu/OfU5tDnQZ38ihXWworpLrQ3L4PWnWHwOMg8x+vKRGpdJAH+PdDdGJOM60LpAyyMSlWxpGAnfDIaPnsGbBEUF7oxoQeM1qBGXrLWTV8280+QMw+atYMrX3TDvOr/izQQkfSBzzPGTAG+AAqBxcC4aBXmuaJC+OIlFxB7tsDpV0Of+2D+c+62axOEi/+qsPDC2k/d/5d1n7jBpgb8Dc68EYLxXlcmUqciugrFWvsA8ECUaokN1sLKD2H67yHvWziuB/SdDK3PdK9f8KBric/9uxudrt+jCvG6kjPfXRK4ehaktoD+f3HBHZ/kdWUintCdmOE2feOCe9VH7iv5VZPgpIHlA9oYuPAP7nbsz8e4lvhFf1SI16b1i2DmI7DyA0jOgL5/hC4/0yiB0uApwAF2bXItu8UTIbExXPRI6GaPhKrXN8aFtg2FeCDgQl0hHl0bv3TB/Z9/uZtwLngQuo6AhBSvKxOJCQ07wA/shc+ehk+egMICN/bzuXdXbxhRY6DfI2XdKSboAkYhHrlN38CsR2D5u5DUxI0Q2PVmSGrsdWUiMaVhBnhxMXw9GWY8BDvXu26SCx8+8glqjXH9sMVF7sRmIAjn36cQr6m879ylmt9MhcQ06DUKuv9Sd0+KHETDC/B1c2HavbBhMbTsGPk1w8bAxY+5ywvn/M21xM//XdTKbRC2rnKzvX89GeIaQc//hrNu04QKIofRcAJ86yr48AH3tTytFQz6XzeAfzRuyAkEYOATrk989l9cS/y8UZHvt77btgZmPwZfvgrBBBfaPUZCSobXlYn4Qv0P8L0/wsd/dYMaBROg9+/hrFshITm6xwkE4KdPuRCf9Yhrife6O7rHqC+257hZcJa87N6nbje7gabSWnhdmYiv1N8AL9wPC19wX833bodOQ93JsLRjau+YgQBc8nfXJz7zf9zznnfW3vH8ZucG18206CXX9dR5uHt/Grf0ujIRX6p/AW4tfPsefHA/bFsF7c6Dvv8Dx5xWN8cPBOGyZ1xLfMbDroV5zu11c+xYtWsTfPI4LBzvrtrpdL0L7qbHel2ZiK/VrwDfsASm/c7dYp3RAa6dDCdeWPdXhQSCcNlYF1YfPgCBODj7trqtIRbs3uKuzpn/PBTth47XuMs0j8r0ujKReqF+BPiO9W6C2i9fc1cuDPgbnDkMgh7+esE4GDTOtcSn/85NKnDWLd7VU5f2bHPXxs/7Xyjc604W9/rNkV+mKSKH5O8A35cPnz7pwsIWQY9fu6/mSU28rswJxrm5F4uLYNo9rmXe7Wavq6o9e7fD58+4kRv358Opg9213M3be12ZSL3kzwAvLoIlr7hWd/4m+MlguOCB2PxqHoyHK8bD5GHwr9+4lnjXn3tdVXQV7HSz4Mx9GvbtgJMvgfPugRaneF2ZSL3mvwBfNdMNOLVpKbTp4gacOrar11UdWjAerpgAk2+E9+9yLfHs//K6qsjty3eXZ859yl2u2WGAu/695eleVybSIPgnwPO+g+n3wYpp0LSta9X+ZLB/bluPS3ATDrx+PfzzDtcS7zzM66pqpmAnLHgePhvjxko/sa9rcZcMuSsidcIfAf7xX9wYGQkpcMFDbtApP44BHZcIV02E166Dd0e6SwzPvN7rqqpvzzbXVTLvWSjYAcf3cS3uWP8GJFJP+SPAj8qC7OGulef326zjEl23z2vXwju/ct0pHa/1uqpDy9/sRm1c8II7OXnSQHeyWC1uEU/5I8BPv9L9qS/ik+Dql+HVa+Aft7julDOu9rqqynbkwqdPuanliva7Lqued+rkpEiM8EeA10fxjeDqV+DVq+Afv3TdKbHyIbVtjbtzcskrgHXzgZ5zB2Sc4HVlIhJGAe6lhGS45nV4ZQhMHeHGTjn1cu/qyfsO5ox2w7oG4qDzjW50wKZtvatJRA5KAe61hGS49nV4+Up48+euO+Ung+q2ho1fwZzHYNk77ptB91/C2b+q3YG/RCRiCvBYkJAC174BL18BU37mulNOuaT2j5uzwAX3f/7t5gLteSd0vwVS0mv/2CISMQV4rEhMhesmw6TLYcpwuPIlOHlg9I9jLaz9xI3HveZjaNTMDbPb5eeaukzEZxTgsSQxDa6bAhMHuVvvr5oIHfpHZ9/WwsoP3Qw4OZ9Dags3zG7n4e7DQ0R8JwrziUlUJTWG699y45e/fj38Z1pk+ysudtPIjevlumh25Lo5PEd+6fq5Fd4ivqUAj0VJTeD6qdDiJ/D6UFjxwZHvo6gQvpoMY892+9i3Cy55Gn692A2mFd8o+nWLSJ1SgMeqRk1diDc/yd16v/LD6m1XuB++mAhjusBbN7llg5+HWxe42/bjEmqtZBGpWxEFuDGmqTFmijHmW2PMcmPMWdEqTHCTU9zwNmS0dyG+aubB1z1QAPOfg7+fCe/c5vrTr5oEv5zrbhDycnILEakVkf6rfhL4t7X2CmNMAhDlqd6lNMRf+im8erW73LBdr7LX9+XDogluUov8TXBsNxj4OJxwgX9GahSRGqlxgBtjGgPnAsMArLX7gf3RKUvKSUmHG99xIf7KVTB0CrQ4FRY852a/2bsNsnrB5S9A5jkKbpEGwlhra7ahMR2BccAy4AxgETDSWru7wnojgBEAbdu27bxu3bpI6m3Y8vPgpYGw/Xt3q/u+ndC+H/S8C47t4nV1IlJLjDGLrLXZlZZHEODZwOdAD2vtPGPMk8BOa+19B9smOzvbLly4sEbHk5D8ze6qkrRj3J2TLc/wuiIRqWUHC/BI+sBzgVxr7bzQ8ynAqAj2J9WRejT8bLrXVYhIDKjxVSjW2h+AHGNMh9CiPrjuFBERqQORXoXyK+Dl0BUoq4HhkZckIiLVEVGAW2uXAJX6ZUREpPbpTkwREZ9SgIuI+JQCXETEpxTgIiI+pQAXEfEpBbiIiE8pwEVEfEoBLiLiUwpwERGfUoCLiPiUAlxExKcU4CIiPqUAFxHxKQW4iIhPKcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSnFOAiIj6lABcR8SkFuIiITynARUR8SgEuIuJTCnAREZ9SgIuI+JQCXETEpyIOcGNM0Biz2Bjzz2gUJCIi1RONFvhIYHkU9iMiIkcgogA3xrQBBgDPR6ccERGprkhb4E8AvwGKD7aCMWaEMWahMWZhXl5ehIcTEZESNQ5wY8xAYLO1dtGh1rPWjrPWZltrs5s3b17Tw4mISAWRtMB7AJcYY9YCrwHnG2MmRaUqERE5rBoHuLX2HmttG2ttJnA18JG1dmjUKhMRkUPSdeAiIj4VF42dWGtnAbOisS8REaketcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSnFOAiIj6lABcR8SkFuIiITynARUR8SgEuIuJTCnAREZ9SgIuI+JQCXETEpxTgIiI+pQAXEfEpBbiIiE8pwEVEfEoBLiLiUwpwERGfUoCLiPiUAlxExKcU4CIiPqUAFxHxKQW4iIhPKcBFRHxKAS4i4lM1DnBjzLHGmJnGmOXGmG+MMSOjWZiIiBxaXATbFgJ3Wmu/MMakAYuMMR9Ya5dFqTYRETmEGrfArbUbrbVfhB7vApYDraNVmIiIHFpU+sCNMZlAJ2BeNPYnIiKHF3GAG2NSgTeB2621O6t4fYQxZqExZmFeXl6khxMRkZCIAtwYE48L75ettW9VtY61dpy1Nttam928efNIDiciImEiuQrFAC8Ay621o6NXkoiIVEckLfAewPXA+caYJaE/F0epLhEROYwaX0Zorf0EMFGsRUREjoDuxBQR8SkFuIiITynARUR8SgEuIuJTCnAREZ9SgIuI+JQCXETEpxTgIiI+pQAXEfEpBbiIiE8pwEVEfEoBLiLiUwpwERGfUoCLiPiUAlxExKcU4CIiPqUAFxHxKQW4iIhPKcBFRHxKAS4i4lM1ntS4Li1dv4Mt+ftol5FK66MaEQxoLmUREV8E+Mvz1vHq/BwA4oOGts2SycpIITM9hazmKWSFfrZISyKgcBeRBsIXAX5X3w4M6tSGNVvyWbNlD2u37GbNlt3MWbGFfYXFpeslxQdcqGekkJnhfpYEfUZqAsYo3EWk/vBFgKenJpKemkjXrGbllhcXWzbuLCgN9DVbdrN2y26++2EXHyzbRGGxLV03LTGuNNTdz2SyMlLJSk+hSXJ8Xf9KIiIR80WAH0wgYGjdtBGtmzaixwkZ5V4rLCpm/fa9rA6FeknAL875kXe/2oAty3aapSSQmZ5MZkYK7cJa75npKaQk+votEpF6rN6mU1wwwHHpKRyXngIdyr+2r7CInG17WJ23m7Vby8J97sqtvPXF+nLrHp2WWNoVk5WRQkZqIvFxARKChvhggIS4APFB9ychGCA+zpQ9DgaID5rSdRKCAfXRi0jU1NsAP5TEuCAnHJ3GCUenVXptz/5C1m7Z47pjtu4uDfkPlm1i6+79ER87GDDEByuEfFjol30gVFwn9GEQeh4XNASNIVjyMxD6U8WyuIAhUPIzfN2wZXHB0M9AgEAA4gIBggEIBgIEjalyWTBoCBhCr4f2bQwmEFoW2q50uUHnIUSiKKIAN8b0A54EgsDz1tpHo1KVh5IT4jilVWNOadW40ms79h5g+579HCgqZn+h5UBRsXtcVMyBIsuBwpLHxewvDC0LX6fCNm4dt557vfzz/H2F7nmhLbf+/qJiioptuT/h/f2xLGAIBbsJhTxl4R8wZa+HnhtD6QdTyeNA2IeDW162rTHl92FKHxN6HvZ6oGT98NcPsb6psH6givUpW8e9Vvl5IPQhVn6f7mf4/gxhzwPln4evB5Xrx/1Xbllo1Qp1ggnbh6lwbGMqH7dkW1PFMYAK65atV/HYhrL9E/ZehS+vWEOlbRt4g6DGAW6MCQJjgAuBXGCBMeYda+2yaBUXa5o0iqdJo9g94VlcbCmyYcFuLUVFFZaFBX6xrXpZYVHoZ7GlOLQ8fJ9ly4optlBUbLGhfRVbKLYl+w49Di0vKn3s9mND21Zcx5bWhXtsQ/sNq7n8cdy+Sp67morD1nH7ccdxy8LXt2HrVXytbNuy3+Vg65f8TlK3DhbslT4IKAv8kg+Uih9G4dtRbrvQsara1yE+ZAg7ziODT690IUakImmBdwVWWmtXAxhjXgMuBeptgMe6QMAQwBAf9LqShs1W+FCwlD0PX074hwhlr1f8cLEV1rOlHyyHWw+g/Lo2rJ6SxyUfUrZc7ZWXlaux3HL3wVjyesnvVVJD6fHcm1Nh32X7otL+w2s9yPIKdZRbp4r6qbB96f+vKvfr3r+y9ao4Bu6JrbSf8vvGQkpi9P9hRhLgrYGcsOe5QLeKKxljRgAjANq2bRvB4UT8obQrhIb99V5qXyRjoVT1t7PSF0hr7Thrbba1Nrt58+YRHE5ERMJFEuC5wLFhz9sAGyIrR0REqiuSAF8AnGiMyTLGJABXA+9EpywRETmcGveBW2sLjTG3AdNwlxGOt9Z+E7XKRETkkCK6Dtxa+z7wfpRqERGRI6AJHUREfEoBLiLiUwpwERGfMiV3QNXJwYzJA9bVcPMMYEsUy/E7vR9l9F6Up/ejvPrwfhxnra10I02dBngkjDELrbXZXtcRK/R+lNF7UZ7ej/Lq8/uhLhQREZ9SgIuI+JSfAnyc1wXEGL0fZfRelKf3o7x6+374pg9cRETK81MLXEREwijARUR8yhcBbozpZ4z5zhiz0hgzyut6vGKMOdYYM9MYs9wY840xZqTXNcUCY0zQGLPYGPNPr2vxmjGmqTFmijHm29Dfk7O8rskrxpg7Qv9OlhpjXjXGJHldU7TFfICHzb3ZHzgFuMYYc4q3VXmmELjTWnsy0B24tQG/F+FGAsu9LiJGPAn821p7EnAGDfR9Mca0Bn4NZFtrT8WNmHq1t1VFX8wHOGFzb1pr9wMlc282ONbajdbaL0KPd+H+cbb2tipvGWPaAAOA572uxWvGmMbAucALANba/dba7Z4W5a04oJExJg5Iph5OOOOHAK9q7s0GHVoAxphMoBMwz+NSvPYE8Bug2OM6YkE7IA+YEOpSet4Yk+J1UV6w1q4HHgO+BzYCO6y1072tKvr8EODVmnuzITHGpAJvArdba3d6XY9XjDEDgc3W2kVe1xIj4oAzgbHW2k7AbqBBnjMyxhyF+6aeBbQCUowxQ72tKvr8EOCaezOMMSYeF94vW2vf8roej/UALjHGrMV1rZ1vjJnkbUmeygVyrbUl38qm4AK9IboAWGOtzbPWHgDeAs72uKao80OAa+7NEGOMwfVvLrfWjva6Hq9Za++x1rax1mbi/l58ZK2td62s6rLW/gDkGGM6hBb1AZZ5WJKXvge6G2OSQ/9u+lAPT+hGNKVaXdDcm+X0AK4HvjbGLAktuzc0tZ0IwK+Al0ONndXAcI/r8YS1dp4xZgrwBe7qrcXUw1vqdSu9iIhP+aELRUREqqAAFxHxKQW4iIhPKcBFRHxKAS4i4lMKcBERn1KAi4j41P8DX5Ya6LDupIMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss and acc\n",
    "plot_loss(train_loss_history, test_loss_history, 'cvae')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
