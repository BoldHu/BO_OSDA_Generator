{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import rdkit\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.enumerator import SmilesEnumerator\n",
    "from utils.build_vocab import WordVocab\n",
    "from datasets.data_loader import Contrastive_Seq2seqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocab size is : 45\n",
      "the total num of charset is : 45\n"
     ]
    }
   ],
   "source": [
    "train_smiles = read_strings('./data/train_smiles.csv', idx=False)\n",
    "train_zeo = read_vec('./data/train_zeo.csv', idx=False)\n",
    "train_syn = read_vec('./data/train_syn.csv', idx=False)\n",
    "train_codes = read_strings('./data/train_codes.csv', idx=False)\n",
    "test_smiles = read_strings('./data/test_smiles.csv', idx=False)\n",
    "test_zeo = read_vec('./data/test_zeo.csv', idx=False)\n",
    "test_syn = read_vec('./data/test_syn.csv', idx=False)\n",
    "test_codes = read_strings('./data/test_codes.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(144938, 1)\n",
      "(144938, 7)\n",
      "(144938, 17)\n",
      "(144938, 1)\n",
      "[['n1(C)c(C)[n+](cc1)Cc1ccccc1C[n+]1ccn(C)c1C']\n",
      " ['Cn1cc[n+](Cc2ccccc2C[n+]2ccn(C)c2C)c1C']\n",
      " ['Cc1n(C)cc[n+]1Cc1c(cccc1)C[n+]1ccn(c1C)C']\n",
      " ['c1ccc(c(c1)C[n+]1c(C)n(C)cc1)C[n+]1ccn(c1C)C']\n",
      " ['[n+]1(c(n(C)cc1)C)Cc1c(C[n+]2ccn(c2C)C)cccc1']]\n",
      "[['0.5746268656716419' '0.25' '1.0' '0.06857614477673278'\n",
      "  '0.45069185152646607' '0.27416454832909665' '0.19443238886477773']\n",
      " ['0.5746268656716419' '0.25' '1.0' '0.06857614477673278'\n",
      "  '0.45069185152646607' '0.27416454832909665' '0.19443238886477773']\n",
      " ['0.5746268656716419' '0.25' '1.0' '0.06857614477673278'\n",
      "  '0.45069185152646607' '0.27416454832909665' '0.19443238886477773']\n",
      " ['0.5746268656716419' '0.25' '1.0' '0.06857614477673278'\n",
      "  '0.45069185152646607' '0.27416454832909665' '0.19443238886477773']\n",
      " ['0.5746268656716419' '0.25' '1.0' '0.06857614477673278'\n",
      "  '0.45069185152646607' '0.27416454832909665' '0.19443238886477773']]\n",
      "[['1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0'\n",
      "  '0.0' '0.0' '0.0' '0.0' '0.0']\n",
      " ['1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0'\n",
      "  '0.0' '0.0' '0.0' '0.0' '0.0']\n",
      " ['1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0'\n",
      "  '0.0' '0.0' '0.0' '0.0' '0.0']\n",
      " ['1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0'\n",
      "  '0.0' '0.0' '0.0' '0.0' '0.0']\n",
      " ['1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0'\n",
      "  '0.0' '0.0' '0.0' '0.0' '0.0']]\n",
      "[['*BEA']\n",
      " ['*BEA']\n",
      " ['*BEA']\n",
      " ['*BEA']\n",
      " ['*BEA']]\n"
     ]
    }
   ],
   "source": [
    "print(type(train_smiles))\n",
    "print(type(train_zeo))\n",
    "print(type(train_syn))\n",
    "print(type(train_codes))\n",
    "print(train_smiles.shape)\n",
    "print(train_zeo.shape)\n",
    "print(train_syn.shape)\n",
    "print(train_codes.shape)\n",
    "print(train_smiles[:5])\n",
    "print(train_zeo[:5])\n",
    "print(train_syn[:5])\n",
    "print(train_codes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[['0.5746268656716419', '0.25', '1.0', '0.06857614477673278', '0.45069185152646607', '0.27416454832909665', '0.19443238886477773'], ['0.5746268656716419', '0.25', '1.0', '0.06857614477673278', '0.45069185152646607', '0.27416454832909665', '0.19443238886477773'], ['0.5746268656716419', '0.25', '1.0', '0.06857614477673278', '0.45069185152646607', '0.27416454832909665', '0.19443238886477773'], ['0.5746268656716419', '0.25', '1.0', '0.06857614477673278', '0.45069185152646607', '0.27416454832909665', '0.19443238886477773'], ['0.5746268656716419', '0.25', '1.0', '0.06857614477673278', '0.45069185152646607', '0.27416454832909665', '0.19443238886477773']]\n",
      "[['1.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0'], ['1.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0'], ['1.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0'], ['1.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0'], ['1.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']]\n"
     ]
    }
   ],
   "source": [
    "# convert zeo and syn to list\n",
    "train_zeo = train_zeo.tolist()\n",
    "train_syn = train_syn.tolist()\n",
    "test_zeo = test_zeo.tolist()\n",
    "test_syn = test_syn.tolist()\n",
    "\n",
    "# check the zeo and syn\n",
    "print(type(train_zeo))\n",
    "print(type(train_syn))\n",
    "print(train_zeo[:5])\n",
    "print(train_syn[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['n1(C)c(C)[n+](cc1)Cc1ccccc1C[n+]1ccn(C)c1C', 'Cn1cc[n+](Cc2ccccc2C[n+]2ccn(C)c2C)c1C', 'Cc1n(C)cc[n+]1Cc1c(cccc1)C[n+]1ccn(c1C)C', 'c1ccc(c(c1)C[n+]1c(C)n(C)cc1)C[n+]1ccn(c1C)C', '[n+]1(c(n(C)cc1)C)Cc1c(C[n+]2ccn(c2C)C)cccc1']\n",
      "['[n+]1(ccn(c1C)C)Cc1c(cccc1)C[n+]1c(n(cc1)C)C', 'c1(C[n+]2ccn(C)c2C)ccccc1C[n+]1c(n(C)cc1)C', 'Cc1[n+](Cc2ccccc2C[n+]2ccn(c2C)C)ccn1C', 'n1(c(C)[n+](Cc2c(cccc2)C[n+]2c(n(cc2)C)C)cc1)C', 'c1(C)[n+](Cc2c(C[n+]3ccn(C)c3C)cccc2)ccn1C']\n"
     ]
    }
   ],
   "source": [
    "# convert SMILES to sequence from np.array to list\n",
    "if type(train_smiles) == np.ndarray:\n",
    "    train_smiles = train_smiles.tolist()\n",
    "    # convert it from [[''], [''], ['']] to ['','','']\n",
    "    train_smiles = [i[0] for i in train_smiles]\n",
    "\n",
    "if type(test_smiles) == np.ndarray:\n",
    "    test_smiles = test_smiles.tolist()\n",
    "    test_smiles = [i[0] for i in test_smiles]\n",
    "\n",
    "print(type(train_smiles))\n",
    "print(type(test_smiles))\n",
    "print(train_smiles[:5])\n",
    "print(test_smiles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['*BEA', '*BEA', '*BEA', '*BEA', '*BEA']\n",
      "['*BEA', '*BEA', '*BEA', '*BEA', '*BEA']\n"
     ]
    }
   ],
   "source": [
    "# convert codes to list from np.array\n",
    "if type(train_codes) == np.ndarray:\n",
    "    train_codes = train_codes.tolist()\n",
    "    train_codes = [i[0] for i in train_codes]\n",
    "\n",
    "if type(test_codes) == np.ndarray:\n",
    "    test_codes = test_codes.tolist()\n",
    "    test_codes = [i[0] for i in test_codes]\n",
    "\n",
    "print(type(train_codes))\n",
    "print(type(test_codes))\n",
    "print(train_codes[:5])\n",
    "print(test_codes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if smiles are valid\n",
    "# # if not, remove them from the dataset including the corresponding zeolite and synthesis vectors, codes\n",
    "# invalid_smiles_index = []\n",
    "# sme = SmilesEnumerator()\n",
    "# for i in range(len(train_smiles)):\n",
    "#     if sme.randomize_smiles(train_smiles[i]) is None:\n",
    "#         invalid_smiles_index.append(i)\n",
    "\n",
    "# # remove invalid smiles and corresponding zeolite and synthesis vectors, codes\n",
    "# # train_smiles is a list, so we can use pop() to remove elements\n",
    "# if invalid_smiles_index:\n",
    "#     for i in range(len(invalid_smiles_index)):\n",
    "#         train_smiles.pop(invalid_smiles_index[i])\n",
    "#     train_zeo = np.delete(train_zeo, invalid_smiles_index, axis=0)\n",
    "#     train_syn = np.delete(train_syn, invalid_smiles_index, axis=0)\n",
    "#     train_codes = np.delete(train_codes, invalid_smiles_index)\n",
    "#     print('Invalid test smiles:', invalid_smiles_index)\n",
    "# else:\n",
    "#     print('No invalid train smiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if test smiles are valid\n",
    "# # if not, remove them from the dataset including the corresponding zeolite and synthesis vectors, codes\n",
    "# invalid_smiles_index = []\n",
    "# sme = SmilesEnumerator()\n",
    "# for i in range(len(test_smiles)):\n",
    "#     if sme.randomize_smiles(test_smiles[i]) is None:\n",
    "#         invalid_smiles_index.append(i)\n",
    "\n",
    "# # remove invalid smiles and corresponding zeolite and synthesis vectors, codes\n",
    "# if invalid_smiles_index:\n",
    "#     for i in range(len(invalid_smiles_index)):\n",
    "#         test_smiles.pop(invalid_smiles_index[i])\n",
    "#     test_zeo = np.delete(test_zeo, invalid_smiles_index, axis=0)\n",
    "#     test_syn = np.delete(test_syn, invalid_smiles_index, axis=0)\n",
    "#     test_codes = np.delete(test_codes, invalid_smiles_index)\n",
    "#     print('Invalid test smiles:', invalid_smiles_index)\n",
    "# else:\n",
    "#     print('No invalid test smiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179742\n",
      "705\n"
     ]
    }
   ],
   "source": [
    "# copy train and test smiles, merge them into one list\n",
    "all_smiles = train_smiles.copy()\n",
    "test_smiles_copy = test_smiles.copy()\n",
    "all_smiles.extend(test_smiles_copy)\n",
    "print(len(all_smiles))\n",
    "\n",
    "# convert all smiles into canonical smiles\n",
    "for i in range(len(all_smiles)):\n",
    "    all_smiles[i] = rdkit.Chem.MolToSmiles(rdkit.Chem.MolFromSmiles(all_smiles[i]), canonical=True)\n",
    "\n",
    "# get unique smiles\n",
    "all_smiles_unique = list(set(all_smiles))\n",
    "print(len(all_smiles_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 220\n",
    "\n",
    "def smiles_to_seq(smile, vocab, seq_len=MAX_LEN):\n",
    "    sm_spaced = split(smile) # Spacing\n",
    "    sm_split = sm_spaced.split()\n",
    "    if len(sm_split)<=MAX_LEN - 2:\n",
    "        # convert to sequence by numpy\n",
    "        content = [vocab.stoi.get(token, vocab.unk_index) for token in smile]\n",
    "        X = [vocab.sos_index] + content + [vocab.eos_index]\n",
    "        padding = [vocab.pad_index]*(seq_len - len(X))\n",
    "        X.extend(padding)\n",
    "        smiles_seq = np.array(X)\n",
    "        return smiles_seq\n",
    "    else:\n",
    "        smile = split(smile).split()\n",
    "        # convert to sequence by numpy\n",
    "        content = [vocab.stoi.get(token, vocab.unk_index) for token in smile]\n",
    "        X = [vocab.sos_index] + content + [vocab.eos_index]\n",
    "        padding = [vocab.pad_index]*(seq_len - len(X))\n",
    "        X.extend(padding)\n",
    "        smiles_seq = np.array(X)\n",
    "        return smiles_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(705, 220)\n",
      "[[ 3  6 11 ...  0  0  0]\n",
      " [ 3  6  6 ...  0  0  0]\n",
      " [ 3  6 18 ...  0  0  0]\n",
      " [ 3  6  6 ...  0  0  0]\n",
      " [ 3  6  6 ...  0  0  0]]\n",
      "(705, 220)\n"
     ]
    }
   ],
   "source": [
    "# convert the unique smiles into sequence\n",
    "all_smiles_seq = []\n",
    "for i in range(len(all_smiles_unique)):\n",
    "    all_smiles_seq.append(smiles_to_seq(all_smiles_unique[i], vocab, seq_len=MAX_LEN))\n",
    "# convert the list into numpy array\n",
    "all_smiles_seq = np.array(all_smiles_seq)\n",
    "print(all_smiles_seq.shape)\n",
    "# save the unique smiles sequence\n",
    "print(all_smiles_seq[:5])\n",
    "np.save('./data/unique_smiles_seq.npy', all_smiles_seq)\n",
    "\n",
    "# read the unique smiles sequence\n",
    "all_smiles_seq = np.load('./data/unique_smiles_seq.npy')\n",
    "print(all_smiles_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dictionary for the unique smiles\n",
    "smiles_dict = {}\n",
    "for i, smiles in enumerate(all_smiles_unique):\n",
    "    smiles_dict[smiles] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the dataset\n",
    "# # code, zeo, syn, smiles, positive_smiles\n",
    "# # positive_smiles: the smiles which is randomized from the original smiles (10)\n",
    "# # canical_smile_index: the index of the original smiles in the all_smiles_seq\n",
    "# train_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "# for i in tqdm(range(len(train_smiles))):\n",
    "#     # get the index of the original smiles in the unique smiles\n",
    "#     canonical_smile = rdkit.Chem.MolToSmiles(rdkit.Chem.MolFromSmiles(train_smiles[i]), canonical=True)\n",
    "#     # get the positive smiles\n",
    "#     positive_smiles = []\n",
    "#     for j in range(10):\n",
    "#         positive_smiles.append(sme.randomize_smiles(canonical_smile))\n",
    "    \n",
    "#     # convert the canonical smiles to sequence\n",
    "#     canonical_smiles_seq = smiles_to_seq(canonical_smile, vocab)\n",
    "    \n",
    "#     # get the canonical smiles row index from the all_smiles_seq by numpy which is the same as the canonical smiles\n",
    "#     canonical_smile_index = np.where(np.all(all_smiles_seq == canonical_smiles_seq, axis=1))[0][0]\n",
    "    \n",
    "#     train_dataset = train_dataset.append({'code': train_codes[i], 'zeo': train_zeo[i], 'syn': train_syn[i],\n",
    "#                                           'smiles': train_smiles[i], 'positive_smiles': positive_smiles, \n",
    "#                                             'canonical_smile_index': canonical_smile_index}, ignore_index=True)\n",
    "    \n",
    "#     # every 10000 samples, save the dataset and clear the dataset\n",
    "#     # avoid the memory error\n",
    "#     if i % 10000 == 0 and i != 0:\n",
    "#         # check if the file exists\n",
    "#         if os.path.exists('./data/train_contrastive_dataset.csv'):\n",
    "#             train_dataset.to_csv('./data/train_contrastive_dataset.csv', mode='a', header=False, index=False)\n",
    "#             # clear the dataset\n",
    "#             train_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "#             print('Save the dataset:', i)\n",
    "#         else:\n",
    "#             train_dataset.to_csv('./data/train_contrastive_dataset.csv', mode='w', header=True, index=False)\n",
    "#             # clear the dataset\n",
    "#             train_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "#             print('Save the dataset:', i)\n",
    "\n",
    "# # save the last dataset\n",
    "# train_dataset.to_csv('./data/train_contrastive_dataset.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "# train_dataset.head(5)\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the test dataset\n",
    "# test_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "# for i in tqdm(range(len(test_smiles))):\n",
    "#     # get the index of the original smiles in the unique smiles\n",
    "#     canonical_smile = rdkit.Chem.MolToSmiles(rdkit.Chem.MolFromSmiles(test_smiles[i]), canonical=True)\n",
    "#     # get the positive smiles\n",
    "#     positive_smiles = []\n",
    "#     for j in range(10):\n",
    "#         positive_smiles.append(sme.randomize_smiles(canonical_smile))\n",
    "    \n",
    "#     # convert the canonical smiles to sequence\n",
    "#     canonical_smiles_seq = smiles_to_seq(canonical_smile, vocab)\n",
    "    \n",
    "#     # get the canonical smiles row index from the all_smiles_seq by numpy which is the same as the canonical smiles\n",
    "#     canonical_smile_index = np.where(np.all(all_smiles_seq == canonical_smiles_seq, axis=1))[0][0]\n",
    "    \n",
    "#     test_dataset = test_dataset.append({'code': test_codes[i], 'zeo': test_zeo[i], 'syn': test_syn[i],\n",
    "#                                           'smiles': test_smiles[i], 'positive_smiles': positive_smiles, \n",
    "#                                             'canonical_smile_index': canonical_smile_index}, ignore_index=True)\n",
    "    \n",
    "#     # every 10000 samples, save the dataset and clear the dataset\n",
    "#     # avoid the memory error\n",
    "#     if i % 10000 == 0 and i != 0:\n",
    "#         # check if the file exists\n",
    "#         if os.path.exists('./data/test_dataset.csv'):\n",
    "#             test_dataset.to_csv('./data/test_contrastive_dataset.csv', mode='a', header=False, index=False)\n",
    "#             test_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "#             print('Save the dataset:', i)\n",
    "#         else:\n",
    "#             test_dataset.to_csv('./data/test_contrastive_dataset.csv', mode='w', header=True, index=False)\n",
    "#             test_dataset = pd.DataFrame(columns=['code', 'zeo', 'syn', 'smiles', 'positive_smiles', 'canonical_smile_index'])\n",
    "#             print('Save the dataset:', i)\n",
    "\n",
    "# # save the last dataset\n",
    "# test_dataset.to_csv('./data/test_contrastive_dataset.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# test_dataset.head(5)\n",
    "# print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144938/144938 [00:00<00:00, 755781.09it/s]\n",
      "100%|██████████| 144938/144938 [00:00<00:00, 565853.57it/s]\n",
      "100%|██████████| 144938/144938 [00:02<00:00, 51455.05it/s]\n",
      "100%|██████████| 14803/14803 [00:00<00:00, 1806519.89it/s]\n",
      "100%|██████████| 14803/14803 [00:00<00:00, 1145688.23it/s]\n",
      "100%|██████████| 14803/14803 [00:00<00:00, 52018.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7])\n",
      "torch.Size([2, 17])\n",
      "torch.Size([2, 220])\n",
      "torch.Size([2, 10, 220])\n",
      "torch.Size([2, 704, 220])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 7])\n",
      "torch.Size([2, 17])\n",
      "torch.Size([2, 220])\n",
      "torch.Size([2, 10, 220])\n",
      "torch.Size([2, 704, 220])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "train_dataset = pd.read_csv('./data/train_contrastive_dataset.csv')\n",
    "test_dataset = pd.read_csv('./data/test_contrastive_dataset.csv')\n",
    "\n",
    "# set the dataset\n",
    "train_dataset = Contrastive_Seq2seqDataset(train_dataset, vocab, MAX_LEN)\n",
    "test_dataset = Contrastive_Seq2seqDataset(test_dataset, vocab, MAX_LEN)\n",
    "\n",
    "# sample the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "for i, (zeo, syn, smiles, positive_smiles, negative_smiles) in enumerate(train_loader):\n",
    "    print(zeo.shape)\n",
    "    print(syn.shape)\n",
    "    print(smiles.shape)\n",
    "    print(positive_smiles.shape)\n",
    "    print(negative_smiles.shape)\n",
    "    \n",
    "    print(type(zeo))\n",
    "    print(type(syn))\n",
    "    print(type(smiles))\n",
    "    print(type(positive_smiles))\n",
    "    print(type(negative_smiles))\n",
    "    break\n",
    "\n",
    "for i, (zeo, syn, smiles, positive_smiles, negative_smiles) in enumerate(test_loader):\n",
    "    print(zeo.shape)\n",
    "    print(syn.shape)\n",
    "    print(smiles.shape)\n",
    "    print(positive_smiles.shape)\n",
    "    print(negative_smiles.shape)\n",
    "\n",
    "    print(type(zeo))\n",
    "    print(type(syn))\n",
    "    print(type(smiles))\n",
    "    print(type(positive_smiles))\n",
    "    print(type(negative_smiles))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
