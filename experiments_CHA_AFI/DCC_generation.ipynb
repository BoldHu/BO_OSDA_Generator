{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working path to the current file\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from models.ddc import SMILESGenerator\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from utils.plot_figures import *\n",
    "from utils.metrics import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ddc(model, start_sequence, condition, max_length, vocab,  device, temperature=0.5, top_k=0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation process for a GPT model.\n",
    "\n",
    "    Args:\n",
    "        model (ddc): The pre-trained GPT model for token generation.\n",
    "        start_sequence (torch.Tensor): The initial sequence to start generation (batch_size, seq_length).\n",
    "        condition_props (torch.Tensor): The conditional property vector (batch_size, num_props).\n",
    "        max_length (int): The maximum length of the generated sequence.\n",
    "        vocab: The vocabulary object for encoding and decoding SMILES strings.\n",
    "        device (torch.device): The device on which to run the generation.\n",
    "        temperature (float): Temperature parameter for sampling; higher values increase randomness.\n",
    "        top_k (int): Limits sampling to top-k logits; if 0, no top-k sampling is applied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated SMILES strings.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    batch_size = start_sequence.size(0)\n",
    "    generated_seq = start_sequence.clone().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # We will iteratively fill positions from [start_len ... seqlen-1]\n",
    "        for cur_len in range(max_length - start_sequence.size(1)):\n",
    "            current_len = generated_seq.size(1)\n",
    "            generated_seq_hot = F.one_hot(generated_seq, num_classes=len(vocab)).float()\n",
    "            # forward pass to get logits\n",
    "            logits = model(condition, generated_seq_hot)\n",
    "            # extract the logits for the next token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # temperature scaling\n",
    "            next_token_logits /= temperature\n",
    "            # top-k sampling\n",
    "            if top_k > 0:\n",
    "                next_token_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            else:\n",
    "                # sample from the distribution\n",
    "                next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            # append the new token to the sequence\n",
    "            generated_seq = torch.cat([generated_seq, next_token], dim=-1)\n",
    "            \n",
    "            # check if all sequences have reached EOS\n",
    "            if all(next_token[i].item() == EOS for i in range(batch_size)):\n",
    "                break\n",
    "    \n",
    "    # Decode the generated sequences into SMILES strings\n",
    "    generated_smiles = []\n",
    "    for seq in generated_seq:\n",
    "        # Convert indices to characters, ignoring padding and start tokens\n",
    "        # check if the generated sequence contains the end token, if meet, stop decoding\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "AFI_smiles = read_strings('./data_AFI/AFI_smiles.csv', idx=False)\n",
    "AFI_zeo = read_vec('./data_AFI/AFI_zeo.csv', idx=False)\n",
    "AFI_syn = read_vec('./data_AFI/AFI_syn.csv', idx=False)\n",
    "CHA_smiles = read_strings('./data_CHA/CHA_smiles.csv', idx=False)\n",
    "CHA_zeo = read_vec('./data_CHA/CHA_zeo.csv', idx=False)\n",
    "CHA_syn = read_vec('./data_CHA/CHA_syn.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "batch_size = 64\n",
    "\n",
    "manual_seed = 42\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the dataset and dataloader\n",
    "AFI_dataset = Seq2seqDataset(AFI_zeo, AFI_syn, AFI_smiles, vocab)\n",
    "CHA_dataset = Seq2seqDataset(CHA_zeo, CHA_syn, CHA_smiles, vocab)\n",
    "AFI_dataloader = DataLoader(AFI_dataset, batch_size=batch_size, shuffle=True)\n",
    "CHA_dataloader = DataLoader(CHA_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_origin = SMILESGenerator(condition_dim=24, lstm_dim=256, dec_layers=3, charset_size=charlen)\n",
    "model_origin.load_state_dict(torch.load('./checkpoints/best_ddc_model.pth'))\n",
    "model_origin.to(device)\n",
    "model_origin.eval()\n",
    "\n",
    "model_cl = SMILESGenerator(condition_dim=24, lstm_dim=256, dec_layers=3, charset_size=charlen)\n",
    "model_cl.load_state_dict(torch.load('./checkpoints/best_ddc_contrastive_model.pth'))\n",
    "model_cl.to(device)\n",
    "model_cl.eval()\n",
    "\n",
    "total = sum(p.numel() for p in model_origin.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))  # print the total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the AFI smiles with the original model\n",
    "generated_smile_origin = []\n",
    "target_smile_origin = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(AFI_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_ddc(model_origin, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_origin.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_origin.extend(tgt_smiles)\n",
    "    \n",
    "# calculate the metrics\n",
    "AFI_validity_rate_origin = validity_rate(generated_smile_origin)\n",
    "AFI_uniqueness_rate_origin = uniqueness_rate(generated_smile_origin)\n",
    "AFI_novelty_rate_origin = novelty_rate(generated_smile_origin, target_smile_origin)\n",
    "AFI_reconstructability_rate_origin = reconstructability_rate(generated_smile_origin, target_smile_origin)\n",
    "AFI_IntDiv_origin = IntDiv(generated_smile_origin)\n",
    "AFI_FCD_score_origin = FCD_score(target_smile_origin, generated_smile_origin)\n",
    "# print the metrics\n",
    "print('AFI_validity_rate_origin: ', AFI_validity_rate_origin)\n",
    "print('AFI_uniqueness_rate_origin: ', AFI_uniqueness_rate_origin)\n",
    "print('AFI_novelty_rate_origin: ', AFI_novelty_rate_origin)\n",
    "print('AFI_reconstructability_rate_origin: ', AFI_reconstructability_rate_origin)\n",
    "print('AFI_IntDiv_origin: ', AFI_IntDiv_origin)\n",
    "print('AFI_FCD_score_origin: ', AFI_FCD_score_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the AFI smiles with the contrastive learning model\n",
    "generated_smile_cl = []\n",
    "target_smile_cl = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(AFI_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_ddc(model_origin, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_cl.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_cl.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "AFI_validity_rate_cl = validity_rate(generated_smile_cl)\n",
    "AFI_uniqueness_rate_cl = uniqueness_rate(generated_smile_cl)\n",
    "AFI_novelty_rate_cl = novelty_rate(generated_smile_cl, target_smile_cl)\n",
    "AFI_reconstructability_rate_cl = reconstructability_rate(generated_smile_cl, target_smile_cl)\n",
    "AFI_IntDiv_cl = IntDiv(generated_smile_cl)\n",
    "AFI_FCD_score_cl = FCD_score(target_smile_cl, generated_smile_cl)\n",
    "# print the metrics\n",
    "print('AFI_validity_rate_cl: ', AFI_validity_rate_cl)\n",
    "print('AFI_uniqueness_rate_cl: ', AFI_uniqueness_rate_cl)\n",
    "print('AFI_novelty_rate_cl: ', AFI_novelty_rate_cl)\n",
    "print('AFI_reconstructability_rate_cl: ', AFI_reconstructability_rate_cl)\n",
    "print('AFI_IntDiv_cl: ', AFI_IntDiv_cl)\n",
    "print('AFI_FCD_score_cl: ', AFI_FCD_score_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the metrics to the folder data_AFI\n",
    "with open('./data_AFI/AFI_generated_ddc_metrics.txt', 'w') as f:\n",
    "    # write the mertics\n",
    "    f.write(f'AFI_validity_rate_origin: {AFI_validity_rate_origin}, AFI_validity_rate_cl: {AFI_validity_rate_cl}\\n')\n",
    "    f.write(f'AFI_uniqueness_rate_origin: {AFI_uniqueness_rate_origin}, AFI_uniqueness_rate_cl: {AFI_uniqueness_rate_cl}\\n')\n",
    "    f.write(f'AFI_novelty_rate_origin: {AFI_novelty_rate_origin}, AFI_novelty_rate_cl: {AFI_novelty_rate_cl}\\n')\n",
    "    f.write(f'AFI_reconstructability_rate_origin: {AFI_reconstructability_rate_origin}, AFI_reconstructability_rate_cl: {AFI_reconstructability_rate_cl}\\n')\n",
    "    f.write(f'AFI_IntDiv_origin: {AFI_IntDiv_origin}, AFI_IntDiv_cl: {AFI_IntDiv_cl}\\n')\n",
    "    f.write(f'AFI_FCD_score_origin: {AFI_FCD_score_origin}, AFI_FCD_score_cl: {AFI_FCD_score_cl}\\n')\n",
    "\n",
    "# write the generated smiles (origin and cl) and target smiles to the folder data_AFI\n",
    "with open('./data_AFI/AFI_generated_ddc_smiles_origin.txt', 'w') as f:\n",
    "    for smiles in range(len(generated_smile_origin)):\n",
    "        f.write(f'origin: {generated_smile_origin[smiles]}, cl: {generated_smile_cl[smiles]}, target: {target_smile_origin[smiles]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the CHA smiles with the original model\n",
    "generated_smile_origin = []\n",
    "target_smile_origin = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(CHA_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_ddc(model_origin, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_origin.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_origin.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "CHA_validity_rate_origin = validity_rate(generated_smile_origin)\n",
    "CHA_uniqueness_rate_origin = uniqueness_rate(generated_smile_origin)\n",
    "CHA_novelty_rate_origin = novelty_rate(generated_smile_origin, target_smile_origin)\n",
    "CHA_reconstructability_rate_origin = reconstructability_rate(generated_smile_origin, target_smile_origin)\n",
    "CHA_IntDiv_origin = IntDiv(generated_smile_origin)\n",
    "CHA_FCD_score_origin = FCD_score(target_smile_origin, generated_smile_origin)\n",
    "# print the metrics\n",
    "print('CHA_validity_rate_origin: ', CHA_validity_rate_origin)\n",
    "print('CHA_uniqueness_rate_origin: ', CHA_uniqueness_rate_origin)\n",
    "print('CHA_novelty_rate_origin: ', CHA_novelty_rate_origin)\n",
    "print('CHA_reconstructability_rate_origin: ', CHA_reconstructability_rate_origin)\n",
    "print('CHA_IntDiv_origin: ', CHA_IntDiv_origin)\n",
    "print('CHA_FCD_score_origin: ', CHA_FCD_score_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the CHA smiles with the contrastive learning model\n",
    "generated_smile_cl = []\n",
    "target_smile_cl = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(CHA_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_ddc(model_cl, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_cl.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_cl.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "CHA_validity_rate_cl = validity_rate(generated_smile_cl)\n",
    "CHA_uniqueness_rate_cl = uniqueness_rate(generated_smile_cl)\n",
    "CHA_novelty_rate_cl = novelty_rate(generated_smile_cl, target_smile_cl)\n",
    "CHA_reconstructability_rate_cl = reconstructability_rate(generated_smile_cl, target_smile_cl)\n",
    "CHA_IntDiv_cl = IntDiv(generated_smile_cl)\n",
    "CHA_FCD_score_cl = FCD_score(target_smile_cl, generated_smile_cl)\n",
    "# print the metrics\n",
    "print('CHA_validity_rate_cl: ', CHA_validity_rate_cl)\n",
    "print('CHA_uniqueness_rate_cl: ', CHA_uniqueness_rate_cl)\n",
    "print('CHA_novelty_rate_cl: ', CHA_novelty_rate_cl)\n",
    "print('CHA_reconstructability_rate_cl: ', CHA_reconstructability_rate_cl)\n",
    "print('CHA_IntDiv_cl: ', CHA_IntDiv_cl)\n",
    "print('CHA_FCD_score_cl: ', CHA_FCD_score_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the metrics to the folder data_CHA\n",
    "with open('./data_CHA/CHA_generated_ddc_metrics.txt', 'w') as f:\n",
    "    # write the mertics\n",
    "    f.write(f'CHA_validity_rate_origin: {CHA_validity_rate_origin}, CHA_validity_rate_cl: {CHA_validity_rate_cl}\\n')\n",
    "    f.write(f'CHA_uniqueness_rate_origin: {CHA_uniqueness_rate_origin}, CHA_uniqueness_rate_cl: {CHA_uniqueness_rate_cl}\\n')\n",
    "    f.write(f'CHA_novelty_rate_origin: {CHA_novelty_rate_origin}, CHA_novelty_rate_cl: {CHA_novelty_rate_cl}\\n')\n",
    "    f.write(f'CHA_reconstructability_rate_origin: {CHA_reconstructability_rate_origin}, CHA_reconstructability_rate_cl: {CHA_reconstructability_rate_cl}\\n')\n",
    "    f.write(f'CHA_IntDiv_origin: {CHA_IntDiv_origin}, CHA_IntDiv_cl: {CHA_IntDiv_cl}\\n')\n",
    "    f.write(f'CHA_FCD_score_origin: {CHA_FCD_score_origin}, CHA_FCD_score_cl: {CHA_FCD_score_cl}\\n')\n",
    "# write the generated smiles (origin and cl) and target smiles to the folder data_CHA\n",
    "with open('./data_CHA/CHA_generated_ddc_smiles_origin.txt', 'w') as f:\n",
    "    for smiles in range(len(generated_smile_origin)):\n",
    "        f.write(f'origin: {generated_smile_origin[smiles]}, cl: {generated_smile_cl[smiles]}, target: {target_smile_origin[smiles]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
