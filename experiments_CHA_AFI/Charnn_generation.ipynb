{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# import the necessary modules\n",
    "from datasets.data_loader import *\n",
    "from utils.utils import *\n",
    "from models.RNN import *\n",
    "from utils.metrics import *\n",
    "from utils.plot_figures import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rnn(model, start_sequence, condition_props, max_length, vocab, device, temperature=1.0, top_k=0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation process for a GPT model.\n",
    "\n",
    "    Args:\n",
    "        model (RNN): The pre-trained GPT model for token generation.\n",
    "        start_sequence (torch.Tensor): The initial sequence to start generation (batch_size, seq_length).\n",
    "        condition_props (torch.Tensor): The conditional property vector (batch_size, num_props).\n",
    "        max_length (int): The maximum length of the generated sequence.\n",
    "        vocab: The vocabulary object for encoding and decoding SMILES strings.\n",
    "        device (torch.device): The device on which to run the generation.\n",
    "        temperature (float): Temperature parameter for sampling; higher values increase randomness.\n",
    "        top_k (int): Limits sampling to top-k logits; if 0, no top-k sampling is applied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated SMILES strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = start_sequence.size(0)\n",
    "    generated_sequences = start_sequence.clone().to(device)  # Clone and move to device\n",
    "\n",
    "    for _ in range(max_length - start_sequence.size(1)):\n",
    "        # Get the current sequence length\n",
    "        current_length = generated_sequences.size(1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits, _ = model(condition_props, generated_sequences)  # (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Extract the logits for the last time step\n",
    "        next_token_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "            mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "            mask.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)\n",
    "            next_token_logits = mask\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Sample from the probability distribution\n",
    "        next_token = torch.multinomial(next_token_probs, num_samples=1)  # (batch_size, 1)\n",
    "        \n",
    "        # Get the most likely next token\n",
    "        # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the generated token to the sequence\n",
    "        generated_sequences = torch.cat([generated_sequences, next_token], dim=1)\n",
    "\n",
    "        # Check if all sequences have reached the end token\n",
    "        if all(next_token[i].item() == EOS for i in range(batch_size)):\n",
    "            break\n",
    "\n",
    "    # Decode the generated sequences into SMILES strings\n",
    "    generated_smiles = []\n",
    "    for seq in generated_sequences:\n",
    "        # Convert indices to characters, ignoring padding and start tokens\n",
    "        # check if the generated sequence contains the end token, if meet, stop decoding\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "AFI_smiles = read_strings('./data_AFI/AFI_smiles.csv', idx=False)\n",
    "AFI_zeo = read_vec('./data_AFI/AFI_zeo.csv', idx=False)\n",
    "AFI_syn = read_vec('./data_AFI/AFI_syn.csv', idx=False)\n",
    "CHA_smiles = read_strings('./data_CHA/CHA_smiles.csv', idx=False)\n",
    "CHA_zeo = read_vec('./data_CHA/CHA_zeo.csv', idx=False)\n",
    "CHA_syn = read_vec('./data_CHA/CHA_syn.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))\n",
    "charlen = len(vocab)\n",
    "print('the total num of charset is :', charlen)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "batch_size = 64\n",
    "\n",
    "manual_seed = 42\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the dataset and dataloader\n",
    "AFI_dataset = Seq2seqDataset(AFI_zeo, AFI_syn, AFI_smiles, vocab)\n",
    "CHA_dataset = Seq2seqDataset(CHA_zeo, CHA_syn, CHA_smiles, vocab)\n",
    "AFI_dataloader = DataLoader(AFI_dataset, batch_size=batch_size, shuffle=True)\n",
    "CHA_dataloader = DataLoader(CHA_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the contrastive learning model and original model from checkpoints\n",
    "model_origin = RNNModel(input_size=charlen, \n",
    "                 synthesis_dim=24, \n",
    "                 embedding_dim=128,\n",
    "                 hidden_size=256,\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 vocab_size=charlen)\n",
    "model_origin.load_state_dict(torch.load('./checkpoints/best_charnn_model.pth'))\n",
    "model_origin.eval()\n",
    "model_origin.to(device)\n",
    "\n",
    "model_cl = RNNModel(input_size=charlen,\n",
    "                    synthesis_dim=24,\n",
    "                    embedding_dim=128,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=3,\n",
    "                    dropout=0,\n",
    "                    vocab_size=charlen)\n",
    "model_cl.load_state_dict(torch.load('./checkpoints/best_charnn_contrastive_model.pth'))\n",
    "model_cl.eval()\n",
    "model_cl.to(device)\n",
    "\n",
    "total = sum(p.numel() for p in model_origin.parameters())\n",
    "print('total parameters: %0.2fM' % (total / 1e6))  # print the total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the AFI smiles with the original model\n",
    "generated_smile_origin = []\n",
    "target_smile_origin = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(AFI_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_rnn(model_origin, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_origin.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_origin.extend(tgt_smiles)\n",
    "    \n",
    "# calculate the metrics\n",
    "AFI_validity_rate_origin = validity_rate(generated_smile_origin)\n",
    "AFI_uniqueness_rate_origin = uniqueness_rate(generated_smile_origin)\n",
    "AFI_novelty_rate_origin = novelty_rate(generated_smile_origin, target_smile_origin)\n",
    "AFI_reconstructability_rate_origin = reconstructability_rate(generated_smile_origin, target_smile_origin)\n",
    "AFI_IntDiv_origin = IntDiv(generated_smile_origin)\n",
    "AFI_FCD_score_origin = FCD_score(target_smile_origin, generated_smile_origin)\n",
    "# print the metrics\n",
    "print('AFI_validity_rate_origin: ', AFI_validity_rate_origin)\n",
    "print('AFI_uniqueness_rate_origin: ', AFI_uniqueness_rate_origin)\n",
    "print('AFI_novelty_rate_origin: ', AFI_novelty_rate_origin)\n",
    "print('AFI_reconstructability_rate_origin: ', AFI_reconstructability_rate_origin)\n",
    "print('AFI_IntDiv_origin: ', AFI_IntDiv_origin)\n",
    "print('AFI_FCD_score_origin: ', AFI_FCD_score_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the AFI smiles with the contrastive learning model\n",
    "generated_smile_cl = []\n",
    "target_smile_cl = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(AFI_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_rnn(model_cl, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_cl.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_cl.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "AFI_validity_rate_cl = validity_rate(generated_smile_cl)\n",
    "AFI_uniqueness_rate_cl = uniqueness_rate(generated_smile_cl)\n",
    "AFI_novelty_rate_cl = novelty_rate(generated_smile_cl, target_smile_cl)\n",
    "AFI_reconstructability_rate_cl = reconstructability_rate(generated_smile_cl, target_smile_cl)\n",
    "AFI_IntDiv_cl = IntDiv(generated_smile_cl)\n",
    "AFI_FCD_score_cl = FCD_score(target_smile_cl, generated_smile_cl)\n",
    "# print the metrics\n",
    "print('AFI_validity_rate_cl: ', AFI_validity_rate_cl)\n",
    "print('AFI_uniqueness_rate_cl: ', AFI_uniqueness_rate_cl)\n",
    "print('AFI_novelty_rate_cl: ', AFI_novelty_rate_cl)\n",
    "print('AFI_reconstructability_rate_cl: ', AFI_reconstructability_rate_cl)\n",
    "print('AFI_IntDiv_cl: ', AFI_IntDiv_cl)\n",
    "print('AFI_FCD_score_cl: ', AFI_FCD_score_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the metrics to the folder data_AFI\n",
    "with open('./data_AFI/AFI_generated_charnn_metrics.txt', 'w') as f:\n",
    "    # write the mertics\n",
    "    f.write(f'AFI_validity_rate_origin: {AFI_validity_rate_origin}, AFI_validity_rate_cl: {AFI_validity_rate_cl}\\n')\n",
    "    f.write(f'AFI_uniqueness_rate_origin: {AFI_uniqueness_rate_origin}, AFI_uniqueness_rate_cl: {AFI_uniqueness_rate_cl}\\n')\n",
    "    f.write(f'AFI_novelty_rate_origin: {AFI_novelty_rate_origin}, AFI_novelty_rate_cl: {AFI_novelty_rate_cl}\\n')\n",
    "    f.write(f'AFI_reconstructability_rate_origin: {AFI_reconstructability_rate_origin}, AFI_reconstructability_rate_cl: {AFI_reconstructability_rate_cl}\\n')\n",
    "    f.write(f'AFI_IntDiv_origin: {AFI_IntDiv_origin}, AFI_IntDiv_cl: {AFI_IntDiv_cl}\\n')\n",
    "    f.write(f'AFI_FCD_score_origin: {AFI_FCD_score_origin}, AFI_FCD_score_cl: {AFI_FCD_score_cl}\\n')\n",
    "\n",
    "# write the generated smiles (origin and cl) and target smiles to the folder data_AFI\n",
    "with open('./data_AFI/AFI_generated_charnn_smiles_origin.txt', 'w') as f:\n",
    "    for smiles in range(len(generated_smile_origin)):\n",
    "        f.write(f'origin: {generated_smile_origin[smiles]}, cl: {generated_smile_cl[smiles]}, target: {target_smile_origin[smiles]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the CHA smiles with the original model\n",
    "generated_smile_origin = []\n",
    "target_smile_origin = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(CHA_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_rnn(model_origin, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_origin.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_origin.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "CHA_validity_rate_origin = validity_rate(generated_smile_origin)\n",
    "CHA_uniqueness_rate_origin = uniqueness_rate(generated_smile_origin)\n",
    "CHA_novelty_rate_origin = novelty_rate(generated_smile_origin, target_smile_origin)\n",
    "CHA_reconstructability_rate_origin = reconstructability_rate(generated_smile_origin, target_smile_origin)\n",
    "CHA_IntDiv_origin = IntDiv(generated_smile_origin)\n",
    "CHA_FCD_score_origin = FCD_score(target_smile_origin, generated_smile_origin)\n",
    "# print the metrics\n",
    "print('CHA_validity_rate_origin: ', CHA_validity_rate_origin)\n",
    "print('CHA_uniqueness_rate_origin: ', CHA_uniqueness_rate_origin)\n",
    "print('CHA_novelty_rate_origin: ', CHA_novelty_rate_origin)\n",
    "print('CHA_reconstructability_rate_origin: ', CHA_reconstructability_rate_origin)\n",
    "print('CHA_IntDiv_origin: ', CHA_IntDiv_origin)\n",
    "print('CHA_FCD_score_origin: ', CHA_FCD_score_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the CHA smiles with the contrastive learning model\n",
    "generated_smile_cl = []\n",
    "target_smile_cl = []\n",
    "for i, (zeo, syn, tgt) in enumerate(tqdm(CHA_dataloader)):\n",
    "    zeo = zeo.to(device)\n",
    "    syn = syn.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    condition_synthesis = torch.cat([zeo, syn], dim=1)\n",
    "    generated_smiles = generate_rnn(model_cl, tgt[:, :10], condition_synthesis, MAX_LEN, vocab, device, 0.5)\n",
    "    generated_smile_cl.extend(generated_smiles)\n",
    "    # convert the tgt to smiles\n",
    "    tgt_smiles = []\n",
    "    for seq in tgt:\n",
    "        smiles = ''\n",
    "        for idx in seq:\n",
    "            if idx.item() == EOS:\n",
    "                break\n",
    "            elif idx.item() != PAD and idx.item() != SOS:\n",
    "                smiles += vocab.itos[idx.item()]\n",
    "        tgt_smiles.append(smiles)\n",
    "    target_smile_cl.extend(tgt_smiles)\n",
    "\n",
    "# calculate the metrics\n",
    "CHA_validity_rate_cl = validity_rate(generated_smile_cl)\n",
    "CHA_uniqueness_rate_cl = uniqueness_rate(generated_smile_cl)\n",
    "CHA_novelty_rate_cl = novelty_rate(generated_smile_cl, target_smile_cl)\n",
    "CHA_reconstructability_rate_cl = reconstructability_rate(generated_smile_cl, target_smile_cl)\n",
    "CHA_IntDiv_cl = IntDiv(generated_smile_cl)\n",
    "CHA_FCD_score_cl = FCD_score(target_smile_cl, generated_smile_cl)\n",
    "# print the metrics\n",
    "print('CHA_validity_rate_cl: ', CHA_validity_rate_cl)\n",
    "print('CHA_uniqueness_rate_cl: ', CHA_uniqueness_rate_cl)\n",
    "print('CHA_novelty_rate_cl: ', CHA_novelty_rate_cl)\n",
    "print('CHA_reconstructability_rate_cl: ', CHA_reconstructability_rate_cl)\n",
    "print('CHA_IntDiv_cl: ', CHA_IntDiv_cl)\n",
    "print('CHA_FCD_score_cl: ', CHA_FCD_score_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the metrics to the folder data_CHA\n",
    "with open('./data_CHA/CHA_generated_charnn_metrics.txt', 'w') as f:\n",
    "    # write the mertics\n",
    "    f.write(f'CHA_validity_rate_origin: {CHA_validity_rate_origin}, CHA_validity_rate_cl: {CHA_validity_rate_cl}\\n')\n",
    "    f.write(f'CHA_uniqueness_rate_origin: {CHA_uniqueness_rate_origin}, CHA_uniqueness_rate_cl: {CHA_uniqueness_rate_cl}\\n')\n",
    "    f.write(f'CHA_novelty_rate_origin: {CHA_novelty_rate_origin}, CHA_novelty_rate_cl: {CHA_novelty_rate_cl}\\n')\n",
    "    f.write(f'CHA_reconstructability_rate_origin: {CHA_reconstructability_rate_origin}, CHA_reconstructability_rate_cl: {CHA_reconstructability_rate_cl}\\n')\n",
    "    f.write(f'CHA_IntDiv_origin: {CHA_IntDiv_origin}, CHA_IntDiv_cl: {CHA_IntDiv_cl}\\n')\n",
    "    f.write(f'CHA_FCD_score_origin: {CHA_FCD_score_origin}, CHA_FCD_score_cl: {CHA_FCD_score_cl}\\n')\n",
    "# write the generated smiles (origin and cl) and target smiles to the folder data_CHA\n",
    "with open('./data_CHA/CHA_generated_charnn_smiles_origin.txt', 'w') as f:\n",
    "    for smiles in range(len(generated_smile_origin)):\n",
    "        f.write(f'origin: {generated_smile_origin[smiles]}, cl: {generated_smile_cl[smiles]}, target: {target_smile_origin[smiles]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
