{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter AFI and CHA Code Zeolite from OSDA_ZEO.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hudongcheng/Desktop/bo_osda_generator\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import custom modules\n",
    "from models.clamer import *\n",
    "from utils.utils import *\n",
    "from datasets.data_loader import *\n",
    "from utils.plot_figures import *\n",
    "from utils.metrics import *\n",
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "save_best_weight_path = './checkpoints/'\n",
    "sample_loss_history = []\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "data_file = r'./data/OSDA_ZEO.xlsx'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature the zeolite\n",
    "def featurize_zeolite(row):\n",
    "    features = [row['FD'], row['max_ring_size'], row['channel_dim'], row['inc_vol'], row['accvol'], row['maxarea'],\n",
    "                row['minarea']]\n",
    "    if any([pd.isnull(x) for x in features]):\n",
    "        # print('Problem: ', row['Code'], 'Missing Information in Data')\n",
    "        return None\n",
    "    else:\n",
    "        return np.array(features, dtype=float)\n",
    "\n",
    "# merge the synthesis conditions\n",
    "def featurize_synthesis(row):\n",
    "    seeds = ['seed', 'SAPO-56 seeds', 'SSZ-57', 'FAU', 'seeded with magadiite', 'seeds']\n",
    "    solvents = ['ethylene glycol', 'hexanol', '2-propanol', 'triethylene glycol', 'triglycol',\n",
    "                'polyethylene glycol', 'n-hexanol', 'glycol', 'propane-1,3-diol', 'butanol',\n",
    "                'glycerol', 'isobutylamine', 'tetraethylene glycol', '1-hexanol',\n",
    "                'sec-butanol', 'iso-butanol', 'ethylene glycol monomethyl ether', 'ethanol']\n",
    "    acids = ['H2SO4', 'acetic acid', 'oxalic acid', 'succinic acid', 'arsenic acid', 'HNO3', 'HCl',\n",
    "             'SO4']\n",
    "    frameworks = ['Co', 'Mn', 'Cu', 'Zn', 'Cd', 'Cr', 'V', 'Ce', 'Nd', 'Sn', 'Zr', 'Ni',\n",
    "                  'S', 'Sm', 'Dy', 'Y', 'La', 'Gd', 'In', 'Nb', 'Te', 'As', 'Hf', 'W',\n",
    "                  'Se']\n",
    "    common_frameworks = ['Si', 'Al', 'P', 'Ge', 'B', 'Ti', 'Ga', 'Fe']\n",
    "    cations = ['Mg', 'Rb', 'Li', 'Cs', 'Sr', 'Ba', 'Be', 'Ca']\n",
    "    common_cations = ['Na', 'K']\n",
    "    bad = ['pictures', 'need access', 'also called azepane', 'SMILES code']\n",
    "    syns = [x.strip() for x in [row['syn1'], row['syn2'], row['syn3'], row['syn4'], row['syn5'],\n",
    "                                row['syn6'], row['syn7'], row['syn8']] if not pd.isnull(x)]\n",
    "    if not syns:\n",
    "        return None\n",
    "    syn_vector = []\n",
    "    for c in common_frameworks:\n",
    "        if c in syns:\n",
    "            syn_vector.append(1)\n",
    "        else:\n",
    "            syn_vector.append(0)\n",
    "    for c in common_cations:\n",
    "        if c in syns:\n",
    "            syn_vector.append(1)\n",
    "        else:\n",
    "            syn_vector.append(0)\n",
    "    if 'F' in syns:\n",
    "        syn_vector.append(1)\n",
    "    else:\n",
    "        syn_vector.append(0)\n",
    "    frame, cat, seed, solv, acid, oth = 0, 0, 0, 0, 0, 0\n",
    "    for s in syns:\n",
    "        if s in frameworks:\n",
    "            frame = 1\n",
    "        elif s in cations:\n",
    "            cat = 1\n",
    "        elif s in seeds:\n",
    "            seed = 1\n",
    "        elif s in solvents:\n",
    "            solv = 1\n",
    "        elif s in acids:\n",
    "            acid = 1\n",
    "        elif s.count(' ') < 2 and s not in bad and len(s) > 2:\n",
    "            oth = 1\n",
    "    syn_vector.extend([frame, cat, seed, solv, acid, oth])\n",
    "    return np.array(syn_vector, dtype=float)\n",
    "\n",
    "# data augmentation\n",
    "def data_augment(augment, data):\n",
    "    smiles_aug, zeo_features_aug, syn_features_aug, codes_aug = [], [], [], []\n",
    "    for i, row in data.iterrows():\n",
    "        if ' + ' not in row['smiles']:  # only look at single-template synthesis\n",
    "            zeo = featurize_zeolite(row=row)\n",
    "            syn = featurize_synthesis(row=row)\n",
    "            if zeo is not None and syn is not None:\n",
    "                if augment:\n",
    "                    new_smiles = []\n",
    "                    m = Chem.MolFromSmiles(row['smiles'])\n",
    "                    for i in range(100):  # randomize smiles string up to 100 times\n",
    "                        try:\n",
    "                            rand_smile = Chem.MolToSmiles(m, canonical=False, doRandom=True, isomericSmiles=False)\n",
    "                            rand_mol = Chem.MolFromSmiles(rand_smile)\n",
    "                            if m is not None and rand_smile not in new_smiles:\n",
    "                                new_smiles.append(rand_smile)\n",
    "                        except:\n",
    "                            # print('Problem:', row['smiles'], 'could not be randomized')\n",
    "                            break\n",
    "                    for smile in new_smiles:\n",
    "                        smiles_aug.append(smile)\n",
    "                        zeo_features_aug.append(zeo)\n",
    "                        syn_features_aug.append(syn)\n",
    "                        codes_aug.append(row['Code'])\n",
    "                else:\n",
    "                    smiles_aug.append(row['smiles'])\n",
    "                    zeo_features_aug.append(zeo)\n",
    "                    syn_features_aug.append(syn)\n",
    "                    codes_aug.append(row['Code'])\n",
    "    return smiles_aug, zeo_features_aug, syn_features_aug, codes_aug\n",
    "\n",
    "# normalize the data\n",
    "def norm(data):\n",
    "    data_max, data_min = np.tile(np.max(data, axis=0), (data.shape[0], 1)), np.tile(\n",
    "        np.min(data, axis=0), (data.shape[0], 1))\n",
    "    data_norm = (data - data_min) / (data_max - data_min)\n",
    "    return data_norm\n",
    "\n",
    "# filter the data\n",
    "def data_sift(smiles, zeo_vecs, syn_vecs, codes):\n",
    "    cant = ['a', 't', ' ', 'i', 'd', 'e', 'f', 'y', 'u']\n",
    "    new_smile, new_zeo, new_syn, new_codes = [], [], [], []\n",
    "    for s, z, v, d in zip(smiles, zeo_vecs, syn_vecs, codes):\n",
    "        if 'Si' in s:\n",
    "            continue\n",
    "        found = False\n",
    "        for c in s:\n",
    "            if c in cant:\n",
    "                found = True\n",
    "        if not found:\n",
    "            new_zeo.append(z)\n",
    "            new_syn.append(v)\n",
    "            new_smile.append(s)\n",
    "            new_codes.append(d)\n",
    "\n",
    "    return new_smile, np.array(new_zeo), np.array(new_syn), new_codes\n",
    "\n",
    "# split the data\n",
    "def data_split(split, unique_codes, smiles, zeo_vectors, syn_vectors, codes):\n",
    "    train_smiles, train_zeo, train_syn, train_codes = [], [], [], []\n",
    "    test_smiles, test_zeo, test_syn, test_codes = [], [], [], []\n",
    "    if split is None:\n",
    "        train_smiles = smiles\n",
    "        train_zeo = zeo_vectors\n",
    "        train_syn = syn_vectors\n",
    "        train_codes = codes\n",
    "    elif split == 'random':\n",
    "        unique_smiles = list(np.unique(smiles))\n",
    "        print(len(unique_smiles))\n",
    "        test_indices = []\n",
    "        random.shuffle(unique_smiles)\n",
    "        test_smiles_unique = unique_smiles[:round(0.2 * len(unique_smiles))]  # 20% held out set\n",
    "        print(len(test_smiles_unique))\n",
    "        for t in test_smiles_unique:\n",
    "            for i, s in enumerate(smiles):\n",
    "                if t == s:\n",
    "                    test_indices.append(i)\n",
    "        print(len(test_indices))\n",
    "        for i, (s, z, v, c) in enumerate(zip(smiles, zeo_vectors, syn_vectors, codes)):\n",
    "            if i in test_indices:\n",
    "                test_smiles.append(s)\n",
    "                test_zeo.append(z)\n",
    "                test_syn.append(v)\n",
    "                test_codes.append(c)\n",
    "            else:\n",
    "                train_smiles.append(s)\n",
    "                train_zeo.append(z)\n",
    "                train_syn.append(v)\n",
    "                train_codes.append(c)\n",
    "    else:\n",
    "        if split not in unique_codes:\n",
    "            print('Problem:', split, 'not a zeolite in the data')\n",
    "        else:\n",
    "            for i, (s, z, v, c) in enumerate(zip(smiles, zeo_vectors, syn_vectors, codes)):\n",
    "                if split == c:\n",
    "                    test_smiles.append(s)\n",
    "                    test_zeo.append(z)\n",
    "                    test_syn.append(v)\n",
    "                    test_codes.append(c)\n",
    "                else:\n",
    "                    train_smiles.append(s)\n",
    "                    train_zeo.append(z)\n",
    "                    train_syn.append(v)\n",
    "                    train_codes.append(c)\n",
    "    return train_smiles, train_zeo, train_syn, train_codes, test_smiles, test_zeo, test_syn, test_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 191\n",
      "len of test_smiles: 317\n",
      "len of test_zeo: 317\n",
      "len of test_syn: 317\n",
      "type of test_smiles: <class 'list'>\n",
      "type of test_zeo: <class 'list'>\n",
      "type of test_syn: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_excel(data_file, engine='openpyxl')\n",
    "smiles_aug, zeo_features_aug, syn_features_aug, codes_aug = data_augment(augment=False, data=data)\n",
    "\n",
    "zeo_features = np.array(zeo_features_aug)\n",
    "zeo_features = norm(zeo_features)\n",
    "\n",
    "smiles, zeo_vectors, syn_vectors, codes = data_sift(smiles_aug, zeo_features, syn_features_aug, codes_aug)\n",
    "print(len(set(smiles)), len(set(codes)))\n",
    "unique_codes = ['AFI']\n",
    "split = \"AFI\"\n",
    "_, _, _, _, test_smiles, test_zeo, test_syn, _ = data_split(split, unique_codes,\n",
    "                                                                        smiles, zeo_vectors, syn_vectors, codes)\n",
    "\n",
    "print('len of test_smiles:', len(test_smiles))\n",
    "print('len of test_zeo:', len(test_zeo))\n",
    "print('len of test_syn:', len(test_syn))\n",
    "\n",
    "print('type of test_smiles:', type(test_smiles))\n",
    "print('type of test_zeo:', type(test_zeo))\n",
    "print('type of test_syn:', type(test_syn))\n",
    "\n",
    "test_smiles = test_smiles * 250\n",
    "test_zeo = test_zeo * 250  # * 121 42\n",
    "test_syn = test_syn * 250  # * 121 42\n",
    "AFI_smiles = test_smiles[0:1000]\n",
    "AFI_zeo = test_zeo[0:1000]\n",
    "AFI_syn = test_syn[0:1000]\n",
    "\n",
    "# save AFI_smiles, AFI_zeo, AFI_syn to data_AFI folder as 3 csv files\n",
    "AFI_smiles = pd.DataFrame(AFI_smiles)\n",
    "AFI_zeo = pd.DataFrame(AFI_zeo)\n",
    "AFI_syn = pd.DataFrame(AFI_syn)\n",
    "AFI_smiles.to_csv('./data_AFI/AFI_smiles.csv', index=False)\n",
    "AFI_zeo.to_csv('./data_AFI/AFI_zeo.csv', index=False)\n",
    "AFI_syn.to_csv('./data_AFI/AFI_syn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocab size is : 45\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "AFI_smiles = read_strings('./data_AFI/AFI_smiles.csv', idx=False)\n",
    "AFI_zeo = read_vec('./data_AFI/AFI_zeo.csv', idx=False)\n",
    "AFI_syn = read_vec('./data_AFI/AFI_syn.csv', idx=False)\n",
    "\n",
    "vocab = WordVocab.load_vocab('./model_hub/vocab.pkl')\n",
    "print('the vocab size is :', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeo: tensor([0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304])\n",
      "syn: tensor([0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "smiles: tensor([ 3,  6, 11,  6,  6,  7, 12,  7,  6,  8,  6, 13,  6,  6,  6,  6,  6, 13,\n",
      "         8,  6,  6,  6, 11,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0])\n",
      "zeo shape: torch.Size([7])\n",
      "syn shape: torch.Size([17])\n",
      "smiles shape: torch.Size([220])\n",
      "type of smiles: <class 'torch.Tensor'>\n",
      "type of zeo: <class 'torch.Tensor'>\n",
      "type of syn: <class 'torch.Tensor'>\n",
      "zeo: tensor([[0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304],\n",
      "        [0.6940, 0.2500, 0.3333, 0.1397, 0.3090, 0.3304, 0.3304]])\n",
      "syn: tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]])\n",
      "smiles: tensor([[ 3,  6,  6,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 12,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  6,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3,  6,  7,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  7,  ...,  0,  0,  0],\n",
      "        [ 3, 12,  7,  ...,  0,  0,  0]])\n",
      "zeo shape: torch.Size([64, 7])\n",
      "syn shape: torch.Size([64, 17])\n",
      "smiles shape: torch.Size([64, 220])\n"
     ]
    }
   ],
   "source": [
    "# create the dataset and dataloader\n",
    "AFI_dataset = Seq2seqDataset(AFI_zeo, AFI_syn, AFI_smiles, vocab)\n",
    "# test Seq2seqDataset\n",
    "for i, (zeo, syn, smiles) in enumerate(AFI_dataset):\n",
    "    print('zeo:', zeo)\n",
    "    print('syn:', syn)\n",
    "    print('smiles:', smiles)\n",
    "    print('zeo shape:', zeo.shape)\n",
    "    print('syn shape:', syn.shape)\n",
    "    print('smiles shape:', smiles.shape)\n",
    "    print('type of smiles:', type(smiles))\n",
    "    print('type of zeo:', type(zeo))\n",
    "    print('type of syn:', type(syn))\n",
    "    break\n",
    "\n",
    "AFI_dataloader = DataLoader(AFI_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test Seq2seqDataset\n",
    "for i, (zeo, syn, smiles) in enumerate(AFI_dataloader):\n",
    "    print('zeo:', zeo)\n",
    "    print('syn:', syn)\n",
    "    print('smiles:', smiles)\n",
    "    print('zeo shape:', zeo.shape)\n",
    "    print('syn shape:', syn.shape)\n",
    "    print('smiles shape:', smiles.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 191\n",
      "len of test_smiles: 243\n",
      "len of test_zeo: 243\n",
      "len of test_syn: 243\n",
      "type of test_smiles: <class 'list'>\n",
      "type of test_zeo: <class 'list'>\n",
      "type of test_syn: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# get CHA code file\n",
    "# read data\n",
    "data = pd.read_excel(data_file, engine='openpyxl')\n",
    "smiles_aug, zeo_features_aug, syn_features_aug, codes_aug = data_augment(augment=False, data=data)\n",
    "\n",
    "zeo_features = np.array(zeo_features_aug)\n",
    "zeo_features = norm(zeo_features)\n",
    "\n",
    "smiles, zeo_vectors, syn_vectors, codes = data_sift(smiles_aug, zeo_features, syn_features_aug, codes_aug)\n",
    "print(len(set(smiles)), len(set(codes)))\n",
    "unique_codes = ['CHA']\n",
    "split = \"CHA\"\n",
    "_, _, _, _, test_smiles, test_zeo, test_syn, _ = data_split(split, unique_codes,\n",
    "                                                                        smiles, zeo_vectors, syn_vectors, codes)\n",
    "\n",
    "print('len of test_smiles:', len(test_smiles))\n",
    "print('len of test_zeo:', len(test_zeo))\n",
    "print('len of test_syn:', len(test_syn))\n",
    "\n",
    "print('type of test_smiles:', type(test_smiles))\n",
    "print('type of test_zeo:', type(test_zeo))\n",
    "print('type of test_syn:', type(test_syn))\n",
    "\n",
    "test_smiles = test_smiles * 250\n",
    "test_zeo = test_zeo * 250  # * 121 42\n",
    "test_syn = test_syn * 250  # * 121 42\n",
    "CHA_smiles = test_smiles[0:1000]\n",
    "CHA_zeo = test_zeo[0:1000]\n",
    "CHA_syn = test_syn[0:1000]\n",
    "\n",
    "# save CHA_smiles, CHA_zeo, CHA_syn to data_CHA by .csv\n",
    "CHA_smiles = pd.DataFrame(CHA_smiles)\n",
    "CHA_zeo = pd.DataFrame(CHA_zeo)\n",
    "CHA_syn = pd.DataFrame(CHA_syn)\n",
    "CHA_smiles.to_csv('./data_CHA/CHA_smiles.csv', index=False)\n",
    "CHA_zeo.to_csv('./data_CHA/CHA_zeo.csv', index=False)\n",
    "CHA_syn.to_csv('./data_CHA/CHA_syn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 191\n",
      "len of test_smiles: 83\n",
      "len of test_zeo: 83\n",
      "len of test_syn: 83\n",
      "type of test_smiles: <class 'list'>\n",
      "type of test_zeo: <class 'list'>\n",
      "type of test_syn: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# get AEI code file\n",
    "# read data\n",
    "data = pd.read_excel(data_file, engine='openpyxl')\n",
    "smiles_aug, zeo_features_aug, syn_features_aug, codes_aug = data_augment(augment=False, data=data)\n",
    "\n",
    "zeo_features = np.array(zeo_features_aug)\n",
    "zeo_features = norm(zeo_features)\n",
    "\n",
    "smiles, zeo_vectors, syn_vectors, codes = data_sift(smiles_aug, zeo_features, syn_features_aug, codes_aug)\n",
    "print(len(set(smiles)), len(set(codes)))\n",
    "unique_codes = ['AEI']\n",
    "split = \"AEI\"\n",
    "_, _, _, _, test_smiles, test_zeo, test_syn, _ = data_split(split, unique_codes,\n",
    "                                                                        smiles, zeo_vectors, syn_vectors, codes)\n",
    "\n",
    "print('len of test_smiles:', len(test_smiles))\n",
    "print('len of test_zeo:', len(test_zeo))\n",
    "print('len of test_syn:', len(test_syn))\n",
    "\n",
    "print('type of test_smiles:', type(test_smiles))\n",
    "print('type of test_zeo:', type(test_zeo))\n",
    "print('type of test_syn:', type(test_syn))\n",
    "\n",
    "test_smiles = test_smiles * 250\n",
    "test_zeo = test_zeo * 250  # * 121 42\n",
    "test_syn = test_syn * 250  # * 121 42\n",
    "AEI_smiles = test_smiles[0:1000]\n",
    "AEI_zeo = test_zeo[0:1000]\n",
    "AEI_syn = test_syn[0:1000]\n",
    "\n",
    "# save AEI_smiles, AEI_zeo, AEI_syn to data_AEI by .csv\n",
    "AEI_smiles = pd.DataFrame(AEI_smiles)\n",
    "AEI_zeo = pd.DataFrame(AEI_zeo)\n",
    "AEI_syn = pd.DataFrame(AEI_syn)\n",
    "AEI_smiles.to_csv('./data_AEI/AEI_smiles.csv', index=False)\n",
    "AEI_zeo.to_csv('./data_AEI/AEI_zeo.csv', index=False)\n",
    "AEI_syn.to_csv('./data_AEI/AEI_syn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
